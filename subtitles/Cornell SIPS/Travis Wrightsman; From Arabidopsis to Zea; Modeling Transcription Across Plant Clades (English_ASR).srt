1
00:00:00,030 --> 00:00:04,339
this is a production of Cornell

2
00:00:01,709 --> 00:00:04,339
University

3
00:00:05,900 --> 00:00:09,740
great thanks for that introduction Jacob

4
00:00:08,298 --> 00:00:10,939
I didn't realize today was cassava

5
00:00:09,740 --> 00:00:12,830
themed but at least I'll be talking

6
00:00:10,939 --> 00:00:16,939
about a rabbi list which was a daikon so

7
00:00:12,830 --> 00:00:19,219
there we go so for today I just want to

8
00:00:16,939 --> 00:00:22,219
generally introduce the concepts behind

9
00:00:19,219 --> 00:00:25,039
one of my PhD projects to use deep

10
00:00:22,219 --> 00:00:27,710
learning models to model transcriptional

11
00:00:25,039 --> 00:00:29,000
processes across rabid OPS's and maize

12
00:00:27,710 --> 00:00:31,160
and also show a little bit of the

13
00:00:29,000 --> 00:00:34,789
preliminary data that I've done from

14
00:00:31,160 --> 00:00:35,808
this work so just to start off we're all

15
00:00:34,789 --> 00:00:37,820
very aware of the different

16
00:00:35,808 --> 00:00:41,828
methodologies used to locate causal si

17
00:00:37,820 --> 00:00:44,238
over the past decades of breeding and

18
00:00:41,829 --> 00:00:45,768
we're familiar with key TL mapping and

19
00:00:44,238 --> 00:00:46,759
also G wasps that we've recently

20
00:00:45,768 --> 00:00:49,640
switched to because of its higher

21
00:00:46,759 --> 00:00:51,768
resolution but we're still limited in

22
00:00:49,640 --> 00:00:53,539
our resolution because of linkage and

23
00:00:51,768 --> 00:00:55,789
it's still very difficult to separate

24
00:00:53,539 --> 00:00:57,920
correlated variance with the actual

25
00:00:55,789 --> 00:01:00,019
causal variance and so the question

26
00:00:57,920 --> 00:01:02,120
always remains how can we increase the

27
00:01:00,018 --> 00:01:03,698
resolution to target those actual causal

28
00:01:02,119 --> 00:01:05,870
barriers

29
00:01:03,698 --> 00:01:08,750
another thing we need to consider is

30
00:01:05,870 --> 00:01:11,120
that at least with QTL and gos you are

31
00:01:08,750 --> 00:01:12,829
directly relating the phenotype with the

32
00:01:11,120 --> 00:01:16,370
genotype data through a linear model and

33
00:01:12,829 --> 00:01:18,289
you're generally ignoring the complex

34
00:01:16,370 --> 00:01:21,620
nonlinear biological processes that

35
00:01:18,290 --> 00:01:24,650
underline these and at least recent

36
00:01:21,620 --> 00:01:27,740
studies have have begun to associates or

37
00:01:24,650 --> 00:01:30,140
the quantitative difference in genotype

38
00:01:27,739 --> 00:01:31,429
with quantitative differences at the RNA

39
00:01:30,140 --> 00:01:33,680
level the protein level and also

40
00:01:31,430 --> 00:01:36,110
metabolite level and today I'll be

41
00:01:33,680 --> 00:01:37,820
focusing on how I'm looking at the

42
00:01:36,109 --> 00:01:43,159
transcriptional level between DNA and

43
00:01:37,819 --> 00:01:44,719
RNA so the flavor of deep learning model

44
00:01:43,159 --> 00:01:46,729
that I'm using is called a convolutional

45
00:01:44,719 --> 00:01:49,189
neural network and you might be familiar

46
00:01:46,730 --> 00:01:50,450
with the term from the context of image

47
00:01:49,189 --> 00:01:52,370
recognition or image classification

48
00:01:50,450 --> 00:01:55,400
where it's the current state of the art

49
00:01:52,370 --> 00:01:56,780
and since convolution is a matrix

50
00:01:55,400 --> 00:01:59,150
operation the first thing we're going to

51
00:01:56,780 --> 00:02:01,250
do so our DNA is converted into a matrix

52
00:01:59,150 --> 00:02:02,990
and so the way we do that is we're

53
00:02:01,250 --> 00:02:05,239
simply going to have a matrix the length

54
00:02:02,989 --> 00:02:07,640
of our sequence and for columns for our

55
00:02:05,239 --> 00:02:09,439
ACTG and we're just going to put ones in

56
00:02:07,640 --> 00:02:11,269
each column corresponding to the bases

57
00:02:09,439 --> 00:02:13,520
so we're going to end up with a sparse

58
00:02:11,269 --> 00:02:14,450
matrix of zeros and ones then

59
00:02:13,520 --> 00:02:16,159
we're gonna do is we're going to take

60
00:02:14,449 --> 00:02:18,949
our orange box here which is basically

61
00:02:16,159 --> 00:02:22,069
our sliding window we also have

62
00:02:18,949 --> 00:02:24,469
corresponding matrices known as filters

63
00:02:22,069 --> 00:02:26,090
which I like to think of as sort of like

64
00:02:24,469 --> 00:02:28,280
position weight matrices for those

65
00:02:26,090 --> 00:02:30,439
familiar with that basically it's a

66
00:02:28,280 --> 00:02:31,909
matrix that weights base occurrence in

67
00:02:30,439 --> 00:02:34,159
certain positions of the window over

68
00:02:31,909 --> 00:02:36,409
others what you're going to do is you're

69
00:02:34,159 --> 00:02:38,539
going to multiply those two matrices

70
00:02:36,409 --> 00:02:40,579
well do a convolution on two of those

71
00:02:38,539 --> 00:02:43,759
matrices and the result you expect to be

72
00:02:40,580 --> 00:02:46,790
higher in the right column which would

73
00:02:43,759 --> 00:02:48,739
be a very dark red color if the two

74
00:02:46,789 --> 00:02:51,829
matrices are more similar so if your

75
00:02:48,740 --> 00:02:53,659
motif is in the window and so the way we

76
00:02:51,830 --> 00:02:55,400
can sort of visualize that is if we

77
00:02:53,659 --> 00:02:56,990
slide along the sequence and now we've

78
00:02:55,400 --> 00:02:59,900
hit a match and we get a high output

79
00:02:56,990 --> 00:03:00,950
value in the right-hand column so the

80
00:02:59,900 --> 00:03:01,909
next thing we're going to do is we're

81
00:03:00,949 --> 00:03:03,859
going to apply what's known as an

82
00:03:01,909 --> 00:03:05,180
activation function and in this case

83
00:03:03,860 --> 00:03:07,310
it's what's known as a real OOP

84
00:03:05,180 --> 00:03:08,960
all the real ooh does is turn all the

85
00:03:07,310 --> 00:03:10,699
negative values to zero and keeps all

86
00:03:08,960 --> 00:03:15,290
your positive values the same way and

87
00:03:10,699 --> 00:03:17,509
this is this is one of the examples of

88
00:03:15,289 --> 00:03:22,069
nonlinear functions that we can use in

89
00:03:17,509 --> 00:03:23,840
in neural networks so one diagram you

90
00:03:22,069 --> 00:03:26,090
may be familiar with if you've seen a

91
00:03:23,840 --> 00:03:27,830
couple of deep learning slides before is

92
00:03:26,090 --> 00:03:29,870
this architecture diagram of the fully

93
00:03:27,830 --> 00:03:32,870
connected layer these are typically done

94
00:03:29,870 --> 00:03:34,939
after your convolutional layers and they

95
00:03:32,870 --> 00:03:36,709
basically consists of a bunch of neurons

96
00:03:34,939 --> 00:03:41,120
that are connected to each these are the

97
00:03:36,709 --> 00:03:43,729
white dots to each of the output labels

98
00:03:41,120 --> 00:03:45,439
from the previous layer with a series of

99
00:03:43,729 --> 00:03:46,759
edges that each have their associated

100
00:03:45,439 --> 00:03:48,590
weights and so all you're going to do

101
00:03:46,759 --> 00:03:50,539
for each of those neurons is take each

102
00:03:48,590 --> 00:03:52,640
of your output data multiply it by the

103
00:03:50,539 --> 00:03:54,079
respective weight sum that result and

104
00:03:52,639 --> 00:03:55,309
tie another activation function that's

105
00:03:54,080 --> 00:03:59,600
your result and you're going to do this

106
00:03:55,310 --> 00:04:01,069
for each layer of your model and then

107
00:03:59,599 --> 00:04:02,659
your final layers your output layer and

108
00:04:01,069 --> 00:04:04,219
that's just going to have what value you

109
00:04:02,659 --> 00:04:09,289
want which in this in my case would be

110
00:04:04,219 --> 00:04:11,569
mRNA expression levels so all of that is

111
00:04:09,289 --> 00:04:12,739
assuming you have all the parameters

112
00:04:11,569 --> 00:04:14,449
that you want you have your filter

113
00:04:12,739 --> 00:04:15,800
matrices estimated you have those

114
00:04:14,449 --> 00:04:17,269
weights s and but what if you're

115
00:04:15,800 --> 00:04:19,250
starting from scratch with a data set

116
00:04:17,269 --> 00:04:20,659
say RNA seek and you want to estimate

117
00:04:19,250 --> 00:04:24,620
those parameters how do you do that

118
00:04:20,660 --> 00:04:26,180
and so we just call it training neural

119
00:04:24,620 --> 00:04:26,689
networks or training models which is

120
00:04:26,180 --> 00:04:28,490
essentially

121
00:04:26,689 --> 00:04:30,680
getting a model to the data and the way

122
00:04:28,490 --> 00:04:33,139
we do that is we divide our input data

123
00:04:30,680 --> 00:04:34,970
up into batches for each those batches

124
00:04:33,139 --> 00:04:36,469
well first we're gonna initialize all

125
00:04:34,970 --> 00:04:38,120
the parameters to small random values

126
00:04:36,470 --> 00:04:39,880
and then we're going to take our first

127
00:04:38,120 --> 00:04:42,079
batch we're gonna make a prediction

128
00:04:39,879 --> 00:04:44,300
which should be pretty bad because it's

129
00:04:42,079 --> 00:04:45,949
just a random model at this point then

130
00:04:44,300 --> 00:04:47,840
we're gonna take the air from that

131
00:04:45,949 --> 00:04:49,459
prediction and the model architecture

132
00:04:47,839 --> 00:04:51,619
and feed that into a training algorithm

133
00:04:49,459 --> 00:04:53,329
and that training algorithm is going to

134
00:04:51,620 --> 00:04:55,490
tell us the direction to update those

135
00:04:53,329 --> 00:04:56,899
parameters to minimize our air we're

136
00:04:55,490 --> 00:04:59,569
going to repeat those steps until we're

137
00:04:56,899 --> 00:05:00,709
out of data that's one epoch and then

138
00:04:59,569 --> 00:05:02,360
normally you would run a couple of

139
00:05:00,709 --> 00:05:06,139
epochs to converge on an accurate model

140
00:05:02,360 --> 00:05:08,960
I do want to emphasize that these

141
00:05:06,139 --> 00:05:10,459
filters I showed earlier they start out

142
00:05:08,959 --> 00:05:13,669
random because you initialize them

143
00:05:10,459 --> 00:05:15,469
randomly and as a consequence of the

144
00:05:13,670 --> 00:05:17,750
training process they gradually become

145
00:05:15,470 --> 00:05:21,140
more defined as motifs if you think of

146
00:05:17,750 --> 00:05:23,360
them as pwms this way matrices and that

147
00:05:21,139 --> 00:05:25,129
is one way that these CN NS can extract

148
00:05:23,360 --> 00:05:26,780
features from your data because you

149
00:05:25,129 --> 00:05:29,089
don't give it these filters in advance

150
00:05:26,779 --> 00:05:33,409
it learns how to accurately predict

151
00:05:29,089 --> 00:05:34,819
expression by learning these motifs so

152
00:05:33,410 --> 00:05:36,350
one of the common criticisms of deep

153
00:05:34,819 --> 00:05:37,790
learning models is that they're even

154
00:05:36,350 --> 00:05:41,300
though they're highly predictive and

155
00:05:37,790 --> 00:05:43,580
they get strong accuracies how do we

156
00:05:41,300 --> 00:05:44,810
interpret what they're actually learning

157
00:05:43,579 --> 00:05:47,389
and they're generally considered to have

158
00:05:44,810 --> 00:05:48,530
low interpret ability but in recent

159
00:05:47,389 --> 00:05:50,209
years there's been a number of

160
00:05:48,529 --> 00:05:51,949
techniques to sort of open up that black

161
00:05:50,209 --> 00:05:54,589
box and interpret what the model is

162
00:05:51,949 --> 00:05:56,810
actually learning one of the example of

163
00:05:54,589 --> 00:05:59,418
this is deep lift and the way deep lift

164
00:05:56,810 --> 00:06:01,189
works simply is by taking a random

165
00:05:59,418 --> 00:06:03,620
background sequence or a reference

166
00:06:01,189 --> 00:06:05,449
sequence it could be some random DNA or

167
00:06:03,620 --> 00:06:08,000
maybe tune to the background frequencies

168
00:06:05,449 --> 00:06:09,889
of your specific organism and then you

169
00:06:08,000 --> 00:06:11,079
take a true positive let's say a gene

170
00:06:09,889 --> 00:06:13,490
that you expect to have high expression

171
00:06:11,079 --> 00:06:14,990
you run those through the model and then

172
00:06:13,490 --> 00:06:17,240
you can directly compare the

173
00:06:14,990 --> 00:06:20,418
intermediate layers of the model and

174
00:06:17,240 --> 00:06:22,879
determine basically what parts of the

175
00:06:20,418 --> 00:06:27,379
input are most predictive of changes in

176
00:06:22,879 --> 00:06:30,019
the output so some

177
00:06:27,379 --> 00:06:32,060
results from or not recent but some

178
00:06:30,019 --> 00:06:33,799
previous results from a postdoc in our

179
00:06:32,060 --> 00:06:35,230
lab hi wang who gave the deep learning

180
00:06:33,800 --> 00:06:39,350
seminar at the beginning of the semester

181
00:06:35,230 --> 00:06:42,020
he took three ideas one was to use the

182
00:06:39,350 --> 00:06:42,980
promoter sequence defined as 15 but

183
00:06:42,019 --> 00:06:45,079
hundred base pairs around the

184
00:06:42,980 --> 00:06:46,700
transcription start site the terminator

185
00:06:45,079 --> 00:06:48,259
sequence is just 1500 based prisoner and

186
00:06:46,699 --> 00:06:50,959
the transcription termination site and

187
00:06:48,259 --> 00:06:53,089
also a combination of the two to predict

188
00:06:50,959 --> 00:06:54,680
expression using a CNN and his results

189
00:06:53,089 --> 00:06:56,269
are on the right and you can basically

190
00:06:54,680 --> 00:06:58,790
see that the promoter and the Terminator

191
00:06:56,269 --> 00:07:01,729
model did the best with our Squared's of

192
00:06:58,790 --> 00:07:05,360
about 0.45 and I do want to emphasize

193
00:07:01,730 --> 00:07:08,780
that though those R Squared's aren't

194
00:07:05,360 --> 00:07:11,270
that high we don't expect to have an R

195
00:07:08,779 --> 00:07:13,129
squared of 1 to predict expression from

196
00:07:11,269 --> 00:07:14,659
sis regulatory sequence because that we

197
00:07:13,129 --> 00:07:16,519
know that biologically that doesn't

198
00:07:14,660 --> 00:07:18,560
explain all the variation there so these

199
00:07:16,519 --> 00:07:22,509
are actually pretty good considering the

200
00:07:18,560 --> 00:07:24,399
input data so another thing

201
00:07:22,509 --> 00:07:27,189
talk about is how we can possibly extend

202
00:07:24,399 --> 00:07:29,289
this mRNA model to generalize different

203
00:07:27,189 --> 00:07:30,879
tissues so we know that the DNA is the

204
00:07:29,290 --> 00:07:32,950
same in different tissues but things

205
00:07:30,879 --> 00:07:34,689
like epigenetics can change but the

206
00:07:32,949 --> 00:07:37,149
model isn't currently taking that into

207
00:07:34,689 --> 00:07:39,129
account and so one way we can think of

208
00:07:37,149 --> 00:07:41,589
is by adding extra columns to this

209
00:07:39,129 --> 00:07:44,170
matrix that correspond to epigenetic

210
00:07:41,589 --> 00:07:45,839
signals that would be present and differ

211
00:07:44,170 --> 00:07:48,189
between tissues so that the model can

212
00:07:45,839 --> 00:07:50,769
learn how to predict differences in

213
00:07:48,189 --> 00:07:53,410
tissue expression and so one example to

214
00:07:50,769 --> 00:07:56,259
be methylation open chromatin signals

215
00:07:53,410 --> 00:08:00,889
histone modifications and also other

216
00:07:56,259 --> 00:08:03,468
epigenetic marks and another concept

217
00:08:00,889 --> 00:08:04,999
touch upon is transfer learning and the

218
00:08:03,468 --> 00:08:08,180
way I like to explain it is teaching an

219
00:08:04,999 --> 00:08:10,400
old dog new tricks you have a model that

220
00:08:08,180 --> 00:08:12,918
you've already trained on one large data

221
00:08:10,399 --> 00:08:15,739
set that achieves high accuracy and now

222
00:08:12,918 --> 00:08:18,109
you want to apply that model in another

223
00:08:15,740 --> 00:08:19,968
related data set that's probably smaller

224
00:08:18,110 --> 00:08:21,710
so if you train a model from scratch you

225
00:08:19,968 --> 00:08:26,718
would not get as high of an accuracy

226
00:08:21,709 --> 00:08:28,128
probably but what you can do is freeze

227
00:08:26,718 --> 00:08:29,598
some of the parameters in the first

228
00:08:28,129 --> 00:08:33,139
model that's been trained on the related

229
00:08:29,598 --> 00:08:35,028
data set and retrain only part of the

230
00:08:33,139 --> 00:08:36,379
model in the smaller data set and you

231
00:08:35,028 --> 00:08:38,689
would expect to achieve slightly higher

232
00:08:36,379 --> 00:08:40,458
accuracy as the smaller data so how does

233
00:08:38,690 --> 00:08:43,089
that apply in our case well if you take

234
00:08:40,458 --> 00:08:46,399
a an organism like rabid offices which

235
00:08:43,089 --> 00:08:49,130
generally has a more diverse and larger

236
00:08:46,399 --> 00:08:51,289
collection of RNA seek data sets you

237
00:08:49,129 --> 00:08:53,600
might say that we could train more

238
00:08:51,289 --> 00:08:57,740
accurate models to predict a rabid

239
00:08:53,600 --> 00:08:59,028
Office's expression and so because we

240
00:08:57,740 --> 00:09:00,589
have a lot more data there but if we

241
00:08:59,028 --> 00:09:02,299
want to transfer into a crop species

242
00:09:00,589 --> 00:09:04,279
that probably doesn't have the diverse

243
00:09:02,299 --> 00:09:06,198
array of data as a rabid abscess well

244
00:09:04,278 --> 00:09:08,689
how could we do that well so if we go

245
00:09:06,198 --> 00:09:10,149
back and think about the convolutional

246
00:09:08,690 --> 00:09:12,860
neural network that I introduced earlier

247
00:09:10,149 --> 00:09:14,958
this part of the network it's sort of

248
00:09:12,860 --> 00:09:17,089
how you learn to it learns to identify

249
00:09:14,958 --> 00:09:19,729
motifs at least of the convolution stage

250
00:09:17,089 --> 00:09:21,740
and then the neural network stage is

251
00:09:19,730 --> 00:09:24,289
really where it learns how these motifs

252
00:09:21,740 --> 00:09:27,799
interact together to regulate and

253
00:09:24,289 --> 00:09:29,539
modulate expression and so if we in this

254
00:09:27,799 --> 00:09:31,819
example make the assumption that between

255
00:09:29,539 --> 00:09:33,708
let's say a rabid office in maze these

256
00:09:31,820 --> 00:09:35,028
motifs are conserved that drive

257
00:09:33,708 --> 00:09:37,129
expression let's say their transcription

258
00:09:35,028 --> 00:09:39,559
factor binding sites that don't change

259
00:09:37,129 --> 00:09:41,750
but the way they relate to each other

260
00:09:39,559 --> 00:09:44,689
and drive high or low expression differs

261
00:09:41,750 --> 00:09:47,269
so what we can do is take an arab adaxes

262
00:09:44,690 --> 00:09:50,089
train model erase let's say the last

263
00:09:47,269 --> 00:09:52,190
layers weights and then retrain it on

264
00:09:50,089 --> 00:09:54,320
May's data just those last parameters

265
00:09:52,190 --> 00:09:56,180
and now we have a model that doesn't

266
00:09:54,320 --> 00:09:58,930
have to relearn how to recognize motifs

267
00:09:56,179 --> 00:10:03,639
but can just simply be tuned to the

268
00:09:58,929 --> 00:10:03,639
species specific connections

269
00:10:04,100 --> 00:10:08,960
and so this is some preliminary results

270
00:10:06,139 --> 00:10:10,730
that I've done in a rabid Oxus using hi

271
00:10:08,960 --> 00:10:13,550
small and so what you're looking at on

272
00:10:10,730 --> 00:10:16,220
the left is a prediction of mean

273
00:10:13,549 --> 00:10:19,839
expression across all tissues for all

274
00:10:16,220 --> 00:10:23,330
genes and rabbit houses and what you can

275
00:10:19,840 --> 00:10:25,759
sort of tell is that it's similar in

276
00:10:23,330 --> 00:10:28,759
accuracy to highs model and if you

277
00:10:25,759 --> 00:10:31,399
compare generally the distributions

278
00:10:28,759 --> 00:10:32,450
across all tissues is the models what I

279
00:10:31,399 --> 00:10:35,750
want to emphasize the models have

280
00:10:32,450 --> 00:10:41,540
comparable results so they do also work

281
00:10:35,750 --> 00:10:43,580
in a rabbit Alice's okay so the

282
00:10:41,539 --> 00:10:46,339
questions that I want to answer with the

283
00:10:43,580 --> 00:10:48,470
rest of this work is basically can these

284
00:10:46,340 --> 00:10:50,990
multi tissue transcriptional models be

285
00:10:48,470 --> 00:10:53,360
trained with epigenetic information is

286
00:10:50,990 --> 00:10:55,789
this a good idea and will it generalize

287
00:10:53,360 --> 00:10:59,060
across tissues another question I'd like

288
00:10:55,789 --> 00:11:00,980
to add answer is can we train models on

289
00:10:59,059 --> 00:11:03,500
one species for example a rabbit OPS's

290
00:11:00,980 --> 00:11:06,590
and then use those as useful starting

291
00:11:03,500 --> 00:11:08,659
models to retrain and retune in other

292
00:11:06,590 --> 00:11:13,129
species to achieve similar accuracies

293
00:11:08,659 --> 00:11:13,829
but with less available data all right

294
00:11:13,129 --> 00:11:15,360
and with

295
00:11:13,830 --> 00:11:17,670
just like to acknowledge the buckler lab

296
00:11:15,360 --> 00:11:19,200
and specifically Heidi and Katherine and

297
00:11:17,669 --> 00:11:21,569
Emery for their discussions on this

298
00:11:19,200 --> 00:11:25,570
project as well as my funding sources

299
00:11:21,570 --> 00:11:31,379
and I'd be happy to take any questions

300
00:11:25,570 --> 00:11:31,379
[Applause]

301
00:11:42,860 --> 00:11:46,800
great questions so the question was am i

302
00:11:45,509 --> 00:11:48,720
training on only one or Abydos this

303
00:11:46,799 --> 00:11:50,370
genotype or all that potentially

304
00:11:48,720 --> 00:11:52,019
thousand and one so currently I'm only

305
00:11:50,370 --> 00:11:54,899
training on Colombia zero the reference

306
00:11:52,019 --> 00:11:58,110
genotype but the plan is to use all

307
00:11:54,899 --> 00:12:02,629
thousand and one genotypes are there any

308
00:11:58,110 --> 00:12:02,629
questions from zoom don't think so

309
00:12:04,039 --> 00:12:14,909
okay well thank you very much Janice of

310
00:12:11,279 --> 00:12:17,600
Cornell University on the web at Cornell

311
00:12:14,909 --> 00:12:17,600
edu

