[SPEAKER_04]: This is a production of Cornell
University.
[SPEAKER_00]: Thank you very much for the intro and for
the welcome.
[SPEAKER_00]: I'm happy to be here.
[SPEAKER_00]: And partly because I've been kind of a sap
about being back on campus after quite a
[SPEAKER_00]: few years.
[SPEAKER_00]: And yeah, it's beautiful to be here.
[SPEAKER_00]: So originally I came to study mechanical
and aerospace engineering.
[SPEAKER_00]: I was hoping to study under Carl Sagan,
but that didn't work out exactly.
[SPEAKER_00]: But I did get to work with Steve Squires
in space sciences.
[SPEAKER_00]: On some early Mars Rover development work
there.
[SPEAKER_00]: So coming from the aerospace robotics
angle, I ended up working on all these
[SPEAKER_00]: tools that had to do with soils and rocks.
[SPEAKER_00]: Like the grinding tool on the Mars Rover,
Spirit and Opportunity.
[SPEAKER_00]: I got to help design and build that and
operate it when it was on Mars.
[SPEAKER_00]: And actually one of them is still working
on Mars now.
[SPEAKER_00]: And that was all because of the Cornell
connections here.
[SPEAKER_00]: And then after a while I thought,
so I was at a private company where we
[SPEAKER_00]: built that for the Rover missions called
Honey Bee Robotics in New York City.
[SPEAKER_00]: And then after a while I thought it would
be nice to maybe take a break from all
[SPEAKER_00]: that rocks and dirt basically.
[SPEAKER_00]: And I figured I'd go to Carnegie Mellon to
do robotic vehicle development work.
[SPEAKER_00]: And my advisor told me once I was there
about this exciting Mars, like a lunar
[SPEAKER_00]: Rover project.
[SPEAKER_00]: I had to develop a new lunar Rover.
[SPEAKER_00]: And he told me what the task was.
[SPEAKER_00]: It was to support drilling down into the
soils.
[SPEAKER_00]: So I started to wonder if this was a
thread in my career that I was kind of
[SPEAKER_00]: inescapable.
[SPEAKER_00]: And then later ended up here now at this
Near Earth Autonomy, which is a spinoff
[SPEAKER_00]: company from Carnegie Mellon from the
Field Robotics Center.
[SPEAKER_00]: And now my major project at the moment is
this agriculture project.
[SPEAKER_00]: And so we end up with a lot of this data.
[SPEAKER_00]: We're making these aerial observations on
the crops and of the soils.
[SPEAKER_00]: So the thread continues.
[SPEAKER_00]: And I'm embracing it.
[SPEAKER_00]: I think it's great to be in the field.
[SPEAKER_00]: I think the other more important thread I
think is that I've enjoyed getting to work
[SPEAKER_00]: on systems that are collecting data for
scientists to make progress in their
[SPEAKER_00]: areas.
[SPEAKER_00]: So this TARA program has been a great
project for us and our company to get
[SPEAKER_00]: deeper into the support of agriculture
from an aerial sensing kind of angle.
[SPEAKER_00]: So I think just to give you a bit more of
just a picture of what we're doing.
[SPEAKER_00]: This TARA program is led by Steve
Kresovich.
[SPEAKER_00]: He's the PI on our project at Clemson.
[SPEAKER_00]: We are the team providing the aerial data
collection and some of the analysis from
[SPEAKER_00]: there.
[SPEAKER_00]: There's also a team at Carnegie Mellon
doing ground vehicles so that robotic
[SPEAKER_00]: vehicles driving through the crops,
collecting data up close to the crops.
[SPEAKER_00]: So combining these data collection
platforms, outputs, and learning more
[SPEAKER_00]: about ways to accelerate the breeding
process.
[SPEAKER_00]: So this is one of the fields.
[SPEAKER_00]: There are these exquisitely planted fields
in South Carolina outside of Clemson.
[SPEAKER_00]: We're seeing hundreds of varieties of
energy sorghum in this one two acre field.
[SPEAKER_00]: And so at this point, I think one of the
ground vehicles was driving, making
[SPEAKER_00]: observations on the ground while we're
flying over.
[SPEAKER_00]: And I hope the two screens doesn't make
anyone motion sick.
[SPEAKER_00]: Even one screen might be enough.
[SPEAKER_00]: So you can see a lot of variation in the
fields.
[SPEAKER_00]: This is definitely not a typical farm when
you're looking at breeding plots.
[SPEAKER_00]: There are a lot of different traits in
these different varieties.
[SPEAKER_00]: This is early.
[SPEAKER_00]: This is the first year out of three for
this project.
[SPEAKER_00]: So we're looking at some of the early
refinements of the breeding.
[SPEAKER_00]: And refinements of the data collection
techniques.
[SPEAKER_00]: So they have two fields in South Carolina.
[SPEAKER_00]: One is right outside of Clemson.
[SPEAKER_00]: And the other is in Florence.
[SPEAKER_00]: So it's more, as you can see, it's a
reddish clay soil.
[SPEAKER_00]: And the Florence side is more coastal
plains, very sandy.
[SPEAKER_00]: So it's nice to be working in two
different conditions.
[SPEAKER_00]: So down for a landing.
[SPEAKER_00]: So just for a little bit of background on
the company I'm at now, we're about 40
[SPEAKER_00]: people.
[SPEAKER_00]: It's about four years old.
[SPEAKER_00]: We're a spinoff from Carnegie Mellon Field
Robotics Center.
[SPEAKER_00]: You can think of us as kind of the
convergence of standard aviation where
[SPEAKER_00]: you're just basically flying.
[SPEAKER_00]: It's good technology, of course,
but it's fairly simple navigation.
[SPEAKER_00]: Usually flying waypoints, straight flight,
fairly straightforward.
[SPEAKER_00]: Converging that with all the kind of
intelligence that you're seeing more and
[SPEAKER_00]: more on driverless cars.
[SPEAKER_00]: The Field Robotics Center is actually a
place that put out a lot of the people who
[SPEAKER_00]: are now at Uber and at Google developing
these driverless cars that came out of the
[SPEAKER_00]: DARPA grand challenges for these vehicles.
[SPEAKER_00]: So our group is a lot of researchers that
came from that area.
[SPEAKER_00]: And so we're converging into this
near-Earth autonomy.
[SPEAKER_00]: So basically the company began where it's
more focused on the navigation,
[SPEAKER_00]: helping aircraft fly on their own down
near the ground where it's difficult.
[SPEAKER_00]: So we work in the field a lot.
[SPEAKER_00]: We work on a lot of different sizes of
helicopters.
[SPEAKER_00]: This is kind of a cool shot, flying up
over Pittsburgh, seeing one of the rivers
[SPEAKER_00]: downtown.
[SPEAKER_00]: This is actually an autonomous flight with
a full-scale helicopter there in the upper
[SPEAKER_00]: left.
[SPEAKER_00]: And we're flying in a lot of different
conditions, even out.
[SPEAKER_00]: We're flying out over Lake Erie lately,
helping a helicopter land onto a barge to
[SPEAKER_00]: show that we could operate from ships.
[SPEAKER_00]: So yeah, we work with full-scale
helicopters and also kind of getting into
[SPEAKER_00]: the more and more where I'm focusing on
the mapping and inspection kind of work is
[SPEAKER_00]: in the smaller, kind of the 50 pounds and
less kind of that kind of category of
[SPEAKER_00]: aircraft where they're like these
quadcopters and hexacopters, that kind of
[SPEAKER_00]: stuff.
[SPEAKER_00]: Now the technology is really improving.
[SPEAKER_00]: They're easier to use.
[SPEAKER_00]: And now the FAA has allowed them to be
used commercially more and more.
[SPEAKER_00]: So it's all working together to become
actually available.
[SPEAKER_00]: And so that's kind of where we're working
for our agricultural work so far.
[SPEAKER_00]: So as a company, we're working a lot on
navigation and mapping and inspection.
[SPEAKER_00]: That's kind of the three kind of
categories.
[SPEAKER_00]: I think we have kind of in our genes as a
company, this history in agriculture,
[SPEAKER_00]: there was a fairly big project the USDA
funded years ago where some of the
[SPEAKER_00]: founders were working on with specialty
crops.
[SPEAKER_00]: This is all ground vehicle based,
but making observations in the field with
[SPEAKER_00]: specialty crops and with nurseries and
landscape industry.
[SPEAKER_00]: And so that's excellent connections,
I think, with Marcel, to Steve.
[SPEAKER_00]: And so now our current work is we're
focused on this accelerated plant breeding
[SPEAKER_00]: using the aerial reconnaissance
essentially to help the breeding process
[SPEAKER_00]: and also trying to spin that off into more
farm management, helping growers.
[SPEAKER_00]: And so you could imagine we're kind of
working with the kind of early adopters at
[SPEAKER_00]: this point, kind of breeding groups that
are fairly high tech.
[SPEAKER_00]: Think of them who function that way.
[SPEAKER_00]: And also some of the maybe larger growers
that would be open to adopting this kind
[SPEAKER_00]: of stuff early.
[SPEAKER_00]: Mostly we work with energy sorghum,
but we're also looking some at maize and
[SPEAKER_00]: grapes.
[SPEAKER_00]: So this boost is our team, it's our
project under the TARA program.
[SPEAKER_00]: There are six different TARA projects.
[SPEAKER_00]: And so it's the Department of Energy's
ARPA-E, it's their DARPA for energy
[SPEAKER_00]: applications.
[SPEAKER_00]: And so I think it's a fantastic program.
[SPEAKER_00]: The goal for TARA as a whole is to
support, to basically accelerate this
[SPEAKER_00]: breeding process to get this really good
potential of energy sorghum as a
[SPEAKER_00]: sustainable biofuel source.
[SPEAKER_00]: And also along the way develop new
techniques for breeding, for accelerated
[SPEAKER_00]: breeding.
[SPEAKER_00]: And looking at high throughput phenotyping
in the field as the main kind of method
[SPEAKER_00]: for that.
[SPEAKER_00]: So we're looking into, you're basically
trying to support this development of
[SPEAKER_00]: predictive algorithms.
[SPEAKER_00]: Get data early in the growing season and
predict the harvest kind of results.
[SPEAKER_00]: Be able to assess a variety really early.
[SPEAKER_00]: And then just get more of these detailed
measurements of the plant physiology.
[SPEAKER_00]: And then focusing a lot on the data
analytics and connecting the genotypes to
[SPEAKER_00]: phenotypes, that kind of mapping.
[SPEAKER_00]: So I don't know if this might give you
motion sickness too potentially,
[SPEAKER_00]: but you can imagine that this first theme,
we're essentially developing these
[SPEAKER_00]: phenotyping hardware systems, getting them
into the field.
[SPEAKER_00]: So that they can collect data both on the
ground and in the air.
[SPEAKER_00]: Taking these results and putting them in
through an analysis pipeline, including
[SPEAKER_00]: machine learning techniques.
[SPEAKER_00]: And then doing this phenotype to genotype
mapping.
[SPEAKER_00]: And feeding that all back into the field.
[SPEAKER_00]: So that we have three growing seasons to
work.
[SPEAKER_00]: So this cycle goes throughout the project.
[SPEAKER_00]: So just thinking about our activity in the
field for this program.
[SPEAKER_00]: We've got these ground vehicles that
Carnegie Mellon are making.
[SPEAKER_00]: They're driving through the rows,
able to do this close proximity kind of
[SPEAKER_00]: sensing.
[SPEAKER_00]: You're right up against the plants.
[SPEAKER_00]: They're doing stereo vision, LIDAR,
other sensor types.
[SPEAKER_00]: And then they're also doing things that
are contact sensing, which I think is
[SPEAKER_00]: pretty novel, where they're actually
interacting physically with the plant.
[SPEAKER_00]: One thing they're focusing on right now is
a penetrometer testing on the strength of
[SPEAKER_00]: the stalks.
[SPEAKER_00]: So it actually is a gripper that visually,
automatically finds a stalk, grabs it,
[SPEAKER_00]: and does a penetrometer test as it goes
through the rows.
[SPEAKER_00]: And then there's also a sensing network
for environmental characteristics on the
[SPEAKER_00]: ground.
[SPEAKER_00]: And then we're the team that's doing the
aerial sensing.
[SPEAKER_00]: So we've got this aircraft flying
overhead, 40 to 60 feet above the ground,
[SPEAKER_00]: mapping the whole field with a custom
sensor package.
[SPEAKER_00]: So that's the bulk of our work,
is developing this custom sensor package.
[SPEAKER_00]: We don't develop the aircraft.
[SPEAKER_00]: We don't develop sensors.
[SPEAKER_00]: We choose aircraft for the purpose.
[SPEAKER_00]: And we choose the sensors for the purpose
and integrate.
[SPEAKER_00]: And then work with that data.
[SPEAKER_00]: Make sure the data can be fused well.
[SPEAKER_00]: It's registered well.
[SPEAKER_00]: Spatially, it's calibrated well in terms
of energetics.
[SPEAKER_00]: And bring that data together and make it
useful.
[SPEAKER_00]: So this is a picture of our first sensor
package for this project.
[SPEAKER_00]: It ended up kind of looking like a
microsatellite or something.
[SPEAKER_00]: This is about six or seven pounds.
[SPEAKER_00]: You could hold it in your hands like this.
[SPEAKER_00]: And it rides on board the aircraft and is
a completely self-contained black box.
[SPEAKER_00]: It's essentially like a very tricked out
GoPro.
[SPEAKER_00]: You hit start.
[SPEAKER_00]: It records from all of its six sensors.
[SPEAKER_00]: And then also, simultaneously,
it is estimating its position and
[SPEAKER_00]: orientation in the world as it goes.
[SPEAKER_00]: So that way, it's not just flying over
with a camera and then stitching together
[SPEAKER_00]: imagery later.
[SPEAKER_00]: That's fairly straightforward.
[SPEAKER_00]: It actually estimates where the sensor
package is and how it's pointing so that
[SPEAKER_00]: you can pull together really explicitly
mapped.
[SPEAKER_00]: So every pixel you know exactly where it
hits on the ground.
[SPEAKER_00]: And so you can make this very accurate
maps, three-dimensionally in some cases.
[SPEAKER_00]: So these are the aircraft we're working
with.
[SPEAKER_00]: This is a 35-pound electric autonomous
helicopter on the left.
[SPEAKER_00]: And then this is a similar weight.
[SPEAKER_00]: There's a hexacopter that we're using now.
[SPEAKER_00]: So we're kind of showing both.
[SPEAKER_00]: There are different categories in terms of
cost and complexity and also performance
[SPEAKER_00]: in some ways.
[SPEAKER_00]: The helicopter can get a lot more coverage
over larger fields faster and fly in more
[SPEAKER_00]: wind conditions.
[SPEAKER_00]: But the hexacopter is really easy to use.
[SPEAKER_00]: And so I think most of the time,
we'll be operating from that.
[SPEAKER_00]: But so we tried to start kind of from
first principles in a way on deciding what
[SPEAKER_00]: was going to be in that sensor package.
[SPEAKER_00]: So we ended up going through that process,
ended up with these four sensing modes.
[SPEAKER_00]: So one, to try to get at the shape of the
crops, kind of geometric properties.
[SPEAKER_00]: We're experimenting with LIDAR.
[SPEAKER_00]: So it's laser scanner there.
[SPEAKER_00]: It's that disk kind of puck-shaped thing
in the front.
[SPEAKER_00]: So it's scanning across the ground with
infrared laser points and building up a 3D
[SPEAKER_00]: cloud.
[SPEAKER_00]: And then to get at kind of color
properties and also geometric properties,
[SPEAKER_00]: we're using visible imaging, just standard
RGB imaging, to look at
[SPEAKER_00]: temperature-related properties and some
stress, plant stress using thermal
[SPEAKER_00]: infrared.
[SPEAKER_00]: So it's just monochromatic, long-wave
infrared.
[SPEAKER_00]: And then to be a little more ambitious,
we also added in hyperspectral camera.
[SPEAKER_00]: So with typical camera, you think of
color.
[SPEAKER_00]: It's three bands, red, green, blue.
[SPEAKER_00]: You could think of them as three separate
layers.
[SPEAKER_00]: If you really look at the sensor on a
color camera, it's actually red,
[SPEAKER_00]: green, blue, and then another green.
[SPEAKER_00]: It's a pattern of four for everything if
you think of it as one pixel.
[SPEAKER_00]: And so you're getting those three separate
bands.
[SPEAKER_00]: For hyperspectral, we're using a camera
that has 100 spectral bands.
[SPEAKER_00]: And they're not overlapping at all.
[SPEAKER_00]: They're all separate, very narrow bands.
[SPEAKER_00]: And it's cutting across the kind of red
and visible and up into the near-infrared,
[SPEAKER_00]: pretty far into the near-infrared.
[SPEAKER_00]: So I think there, we're taking a lot of
inspiration from the remote sensing
[SPEAKER_00]: approaches from satellite, where they have
all these interesting indices.
[SPEAKER_00]: They'll say, this spectral band ratioed
with this spectral band ends up
[SPEAKER_00]: correlating really well with this kind of
plant property.
[SPEAKER_00]: And so there, we're thinking that through
this whole combination, hyperspectral in
[SPEAKER_00]: particular, I think we're going to get to
explore and look for these properties,
[SPEAKER_00]: these predictive things, these
correlations to plant properties,
[SPEAKER_00]: and be able to, I think we'll learn quite
a bit, and then also get to do a second
[SPEAKER_00]: generation in the second year of this
payload, where the sensor suite where
[SPEAKER_00]: it'll be more refined and a bit cheaper
too.
[SPEAKER_00]: So it'll be closer to being
commercialized.
[SPEAKER_00]: So here's a shot of us.
[SPEAKER_00]: It's in the field.
[SPEAKER_00]: This is the Sandier site there in South
Carolina.
[SPEAKER_00]: And there's the sensor package writing
down below on one of the multi-rotor
[SPEAKER_00]: copters.
[SPEAKER_00]: So it's pretty straightforward.
[SPEAKER_00]: It takes just a couple people to operate
in the field.
[SPEAKER_00]: It takes off.
[SPEAKER_00]: And collects data all along, lands.
[SPEAKER_00]: Here's a shot of us.
[SPEAKER_00]: I think it was the last month.
[SPEAKER_00]: I would lower the volume, but I don't know
how.
[SPEAKER_00]: I tried.
[SPEAKER_00]: So you can see it takes off.
[SPEAKER_00]: It's really simple to operate this thing,
even though it's a pretty serious aircraft
[SPEAKER_00]: in a way.
[SPEAKER_00]: I mean, you have to be very careful.
[SPEAKER_00]: We're following all the FAA regulations.
[SPEAKER_00]: We have registered pilots.
[SPEAKER_00]: The guy who's actually piloting this,
he manually made it take off and then
[SPEAKER_00]: flipped it into autonomous mode for it to
do its kind of simple lawn mower kind of
[SPEAKER_00]: pattern over the crops.
[SPEAKER_00]: But he used to fly Apache helicopters for
a long time in South Korea.
[SPEAKER_00]: So that's him there, Spencer.
[SPEAKER_00]: So he's kind of unfazed by this stuff,
luckily.
[SPEAKER_00]: So yeah, the flying portions is easy.
[SPEAKER_00]: As long as you follow all the right
procedures and keep everything legal and
[SPEAKER_00]: safe, it's all very straightforward stuff.
[SPEAKER_00]: It's not novel technology there.
[SPEAKER_00]: It's off the shelf.
[SPEAKER_00]: So all our focus is in making the sensor
suite correct and making the data
[SPEAKER_00]: worthwhile.
[SPEAKER_00]: So here's just a series of frames.
[SPEAKER_00]: So it's all straight down looking cameras.
[SPEAKER_00]: This is just some of the visible data from
last year.
[SPEAKER_00]: This is from the RGB camera.
[SPEAKER_00]: And then just an example of what some of
that looks like once you start to build
[SPEAKER_00]: these orthographic maps from the imagery
to get a sense of kind of resolutions
[SPEAKER_00]: we're getting.
[SPEAKER_00]: This was flying a little on the high side,
about 60 feet above ground level.
[SPEAKER_00]: I was showing Steve some of the weeds on
this shot.
[SPEAKER_00]: And then just a better sense zooming in.
[SPEAKER_00]: So you're getting many pixels across a
leaf, that kind of spatial resolution,
[SPEAKER_00]: seeing color variation, seeing texture in
the panicles of these sorghum plants.
[SPEAKER_00]: Most of the time, we're flying over energy
sorghum, so the panicles aren't there.
[SPEAKER_00]: But in some cases, we are.
[SPEAKER_00]: You're also seeing we'll be able to be
pulling out information about the leaf
[SPEAKER_00]: geometries.
[SPEAKER_00]: We're also seeing some lodging here,
as an example.
[SPEAKER_00]: I think that's something we could probably
pull out.
[SPEAKER_00]: So then another data product is the LiDAR
data.
[SPEAKER_00]: So this is this point cloud.
[SPEAKER_00]: This isn't cleaned up or anything.
[SPEAKER_00]: It's just basically what it sees as it's
flying.
[SPEAKER_00]: And so it's just a real 3D model.
[SPEAKER_00]: It's like flying around in a strange video
game afterwards.
[SPEAKER_00]: You can actually take measurements between
points.
[SPEAKER_00]: So you're essentially able to just,
after the fact, go through and measure
[SPEAKER_00]: geometric properties.
[SPEAKER_00]: So later, you get this 3D model that you
can spin around, map in different ways,
[SPEAKER_00]: process in different ways, and get the 3D
information.
[SPEAKER_00]: So those are some vans, vehicles,
and tents here in that big open aisle.
[SPEAKER_00]: And so you're seeing a lot of the color
variation.
[SPEAKER_00]: It's a color scale is applied just to
visualize the differences in heights.
[SPEAKER_00]: So it's like a heat map for height.
[SPEAKER_00]: So you're seeing a lot of these plots who
clearly are different varieties.
[SPEAKER_00]: And so you're seeing a fair amount of
variation in the height by plot.
[SPEAKER_00]: That's actually one of the phenotype that
we're focusing on these last couple of
[SPEAKER_00]: weeks is cleaning up a way to produce a
good measurement of plant height averaged
[SPEAKER_00]: out by plot.
[SPEAKER_00]: So this data kind of lends itself to that,
to be able to know what your
[SPEAKER_00]: characteristic height is for each plot and
the amount of variation in height in that
[SPEAKER_00]: plot.
[SPEAKER_00]: And getting more and more appreciation for
how that sounds easy but isn't easy.
[SPEAKER_00]: I heard a story of, Steve Kresovich told
us a story of a group with, I think,
[SPEAKER_00]: maybe six pretty respected faculty members
on this one program, NSF program.
[SPEAKER_00]: And multiple of them were in the National
Academy of Sciences.
[SPEAKER_00]: And it took them months to argue out how
to measure the height of the plant.
[SPEAKER_00]: And now I understand that a little bit
better.
[SPEAKER_00]: The idea of if it has a panicle or not,
does that influence your height
[SPEAKER_00]: measurement?
[SPEAKER_00]: Is the leaf pointing up or down?
[SPEAKER_00]: Does that mean the plant is a different
height?
[SPEAKER_00]: Is the wind blowing things?
[SPEAKER_00]: What's your standard method?
[SPEAKER_00]: How a person would do it versus what can
you actually do from the air?
[SPEAKER_00]: So I think we're getting more and more
appreciation for some of these things with
[SPEAKER_00]: the sensitivities.
[SPEAKER_00]: Here's just another shot of the LiDAR
point cloud.
[SPEAKER_00]: These are the vans there in the top for
scale.
[SPEAKER_00]: So as we fly, we're registering the data
using GPS and inertial sensing within the
[SPEAKER_00]: payload.
[SPEAKER_00]: It's completely separate.
[SPEAKER_00]: It just is along for the ride on the
aircraft.
[SPEAKER_00]: It's not interacting with the aircraft
controls.
[SPEAKER_00]: We're also experimenting with targets on
the ground to help tighten up the spatial
[SPEAKER_00]: registration even more.
[SPEAKER_00]: On the calibration, we worked to some
extent with what the manufacturers give us
[SPEAKER_00]: as far as the calibration of cameras.
[SPEAKER_00]: But then we're also doing our own quite a
bit.
[SPEAKER_00]: In some ways, we're redoing and improving
things that the manufacturers do,
[SPEAKER_00]: especially for the energetics.
[SPEAKER_00]: On the hyperspectral camera, we're doing
things where we give different light
[SPEAKER_00]: source inputs.
[SPEAKER_00]: So doing lab calibration, then also field
calibration, laying out tarps in the
[SPEAKER_00]: field, taking off and landing from a
calibration target, more or less.
[SPEAKER_00]: And then we're experimenting some with
shadowing.
[SPEAKER_00]: The intent there is just to make sure that
the data is closely tied to the real
[SPEAKER_00]: physics of the plant.
[SPEAKER_00]: There's real plant properties and aren't
influenced too much by the changes in
[SPEAKER_00]: lighting conditions, which can be pretty
significant.
[SPEAKER_00]: With photography, if you're messing around
with different white balances,
[SPEAKER_00]: you can imagine it being very different.
[SPEAKER_00]: And that's just trying to make things look
appropriate.
[SPEAKER_00]: If you're trying to make physical
deductions, then it's even more important.
[SPEAKER_00]: So with our analytics kind of support,
we've got these foundational things,
[SPEAKER_00]: ways of modeling what the ground shape is
and segmenting into plots and annotating
[SPEAKER_00]: things, producing these raw data products,
like just the standard things that are
[SPEAKER_00]: collected right off the camera.
[SPEAKER_00]: And then these reduced products,
these image maps, 3D models.
[SPEAKER_00]: And then these more derived data products
is where we're starting to put more focus
[SPEAKER_00]: into now, things like phenotypes,
plant traits, trends in these plant
[SPEAKER_00]: traits.
[SPEAKER_00]: And then we'll be supporting the rest of
the team at Clemson, where Steve
[SPEAKER_00]: Kresovich's team is, and the Danforth
Center, where they'll both be doing a lot
[SPEAKER_00]: of the effort of making correlations
between the phenotypes and genotypes and
[SPEAKER_00]: feeding that into the breeding process.
[SPEAKER_00]: So I think I described those fairly well.
[SPEAKER_00]: I think just to get a sense of some of the
other phenotypes we're going after,
[SPEAKER_00]: things like observing, being able to map
out when emergence happens, where,
[SPEAKER_00]: across the field, measuring the plant
height per plot, and then how that changes
[SPEAKER_00]: over time, whether or not there's a
panicle, and things like canopy closure,
[SPEAKER_00]: leaf area index, lodging, if the plants
have fallen over, just other geometric
[SPEAKER_00]: types of things, other stress,
like water stress, or nitrogen stress,
[SPEAKER_00]: or leaf necrosis, and anything we can
learn about the photosynthetic activity.
[SPEAKER_00]: I think from our hyperspectral camera,
we hope to be able to pick that up pretty
[SPEAKER_00]: well.
[SPEAKER_00]: And other hyperspectral indices that could
maybe have novel correlations that we'll
[SPEAKER_00]: find in this project.
[SPEAKER_00]: So in the end, I think once these become
more refined, the hope is that we're
[SPEAKER_00]: giving advice to breeders, feeding into
their analysis chains, and advice for
[SPEAKER_00]: growers, too, how to better manage their
fields, so that they're not just given.
[SPEAKER_00]: Say earlier this week, our people were in
the field in South Carolina.
[SPEAKER_00]: They came back with 1.3 terabytes,
and they only flew I think six times.
[SPEAKER_00]: And so a grower doesn't want that.
[SPEAKER_00]: A lot of us, they'll just be kind of blown
away by that.
[SPEAKER_00]: It's just not useful.
[SPEAKER_00]: I think Steve and I were talking.
[SPEAKER_00]: They're busy already.
[SPEAKER_00]: They're not looking for a boring hobby on
top of their job.
[SPEAKER_00]: So we're trying to refine this software
process so that they're given something
[SPEAKER_00]: that's more like actionable intelligence,
to use a term from that world.
[SPEAKER_00]: So it's just basically, what do I do?
[SPEAKER_00]: Give me advice.
[SPEAKER_00]: And so the ARPA-E program, TERA,
is really consciously pushing us to think
[SPEAKER_00]: about commercialization.
[SPEAKER_00]: We've identified a lot of different
potential ways that things could go.
[SPEAKER_00]: We could transition this technology to
market.
[SPEAKER_00]: In the end, we're collecting a huge amount
of data in the field.
[SPEAKER_00]: I think that could be valuable,
just the straight data.
[SPEAKER_00]: The analysis algorithms could potentially
be licensable.
[SPEAKER_00]: The vehicles and the sensing packages.
[SPEAKER_00]: And then also, you can imagine the whole
integrated phenotyping system of ground
[SPEAKER_00]: and air and analysis.
[SPEAKER_00]: And then I think we could very well make
significant progress on actual new,
[SPEAKER_00]: more refined varieties of energy sorghum
throughout this project.
[SPEAKER_00]: So I think looking ahead a bit,
we're hoping to extend the work.
[SPEAKER_00]: On to other target crops, a lot of our
sensor approaches and analysis approaches
[SPEAKER_00]: aren't completely specialized to energy
sorghum.
[SPEAKER_00]: They could easily be applied to other crop
types or even other areas beyond
[SPEAKER_00]: specifically crops, like surveying for
disease in trees.
[SPEAKER_00]: That's something that has been coming up
in the Pennsylvania area lately in
[SPEAKER_00]: discussions.
[SPEAKER_00]: So we're trying to line up more field
trials with large farms and smaller,
[SPEAKER_00]: innovative farms.
[SPEAKER_00]: We've got people in the area.
[SPEAKER_00]: Pittsburgh is such a robotics mecca that
we get lucky that there's a farmer outside
[SPEAKER_00]: of town who just supports our work.
[SPEAKER_00]: So we get to operate there a lot,
work on his crops.
[SPEAKER_00]: And so it's a nice example.
[SPEAKER_00]: We're going to be doing some breeding
trials in Australia and trying to find a
[SPEAKER_00]: way to stay active during the winter here.
[SPEAKER_00]: So we'll be there outside of Melbourne
this winter.
[SPEAKER_00]: So I think there's some good potentials
there.
[SPEAKER_00]: So this is a three year program with
Terra.
[SPEAKER_00]: And so we're hoping to continue that,
too.
[SPEAKER_00]: There's some potential there once some
funding matching happens.
[SPEAKER_00]: So that could continue the work,
make it more refined, maybe taking some
[SPEAKER_00]: different directions.
[SPEAKER_00]: So yeah, I think it would be great if
anybody had any questions.
[SPEAKER_00]: I'd be happy to answer those.
[SPEAKER_02]: Thank you very much, Paul, for offering
any questions.
[SPEAKER_02]: I have a question.
[SPEAKER_02]: I noticed on the flyover that you showed
us, it was an overcast day.
[SPEAKER_02]: Does it matter?
[SPEAKER_02]: Do you prefer overcast versus sunny or
sunny better?
[SPEAKER_00]: Yeah, overcast is nice, actually.
[SPEAKER_00]: It ends up with much more diffuse light,
less shadowing.
[SPEAKER_00]: Yeah, it's kind of like doing photography.
[SPEAKER_00]: It's counterintuitive.
[SPEAKER_00]: We're trying to just get the processes
right so that we can deal with either
[SPEAKER_00]: conditions.
[SPEAKER_00]: But shadowing is hard to work with.
[SPEAKER_00]: If you want to see down into the crops,
not just the top layer, then it gets
[SPEAKER_00]: tricky.
[SPEAKER_00]: It can muddy things, for sure.
[SPEAKER_00]: You end up with dynamic range issues,
like very well lit leaves and portions of
[SPEAKER_00]: the plant and some part of the scene.
[SPEAKER_00]: And then very low lighting on other
portions.
[SPEAKER_00]: It's hard to get useful data across the
whole range.
[SPEAKER_03]: You can get a sense of the instrument
package when you're flying over.
[SPEAKER_03]: Are you able to focus on particular quite
small parts and get data from those?
[SPEAKER_03]: For example, if you exclude soil,
for example, plants, you're looking at
[SPEAKER_03]: particular portions of the canopy.
[SPEAKER_03]: Can you do that?
[SPEAKER_00]: Yeah, so as far as the data collection
goes, we are just covering the entire
[SPEAKER_00]: field.
[SPEAKER_00]: And we do it from two different
directions.
[SPEAKER_00]: So we'll cover it, say, east to west,
and then go back again north-south.
[SPEAKER_00]: And so you get a fair amount of very
complete coverage.
[SPEAKER_00]: And then you get a fair amount of
penetrating observations, like down
[SPEAKER_00]: through, especially early and mid-season,
down to the soils and lower portions of
[SPEAKER_00]: the plants.
[SPEAKER_00]: And then, yeah, with the analysis,
I think separating out soils from plant
[SPEAKER_00]: material is a fairly, I think,
pretty straightforward thing that we'll be
[SPEAKER_00]: doing soon.
[SPEAKER_00]: And then it's hard to say how far we'll
get to go with breaking out in an
[SPEAKER_00]: automated sort of way different portions
of the plant.
[SPEAKER_00]: But my hope is that we could at least pick
out a panicle and leaves separate of the
[SPEAKER_00]: rest of the scene.
[SPEAKER_03]: So when you do your thermal imaging for
the canopy temperature,
[SPEAKER_03]: what instrument do you use?
Right.
[SPEAKER_00]: Yeah, so it's fairly low resolution.
[SPEAKER_00]: It's an imager from FLIR company.
[SPEAKER_00]: So it's about 640 pixels across.
[SPEAKER_00]: And that's just because of the state of
the art.
[SPEAKER_00]: With color imaging, we have a 12-megapixel
camera on board.
[SPEAKER_00]: But this was a 1-megapixel camera just
because that's kind of the top of what's
[SPEAKER_00]: available right now.
[SPEAKER_00]: And luckily, a lot of the thermal
properties in the field are fairly coarse,
[SPEAKER_00]: spatially speaking.
[SPEAKER_00]: They're fairly coarse things.
[SPEAKER_00]: So I think it's still useful.
[SPEAKER_00]: And so I think early on, say early in the
season, it might be a little bit muddy
[SPEAKER_00]: where pixels are overlapping a lot.
[SPEAKER_00]: It'll be half landing on a leaf and half
on soil.
[SPEAKER_00]: But later on, once the canopy starts to
close, I think it'll be pretty clean
[SPEAKER_00]: sensing of canopy thermal emission.
[SPEAKER_03]: Primary.
[SPEAKER_00]: Yeah, primary.
[SPEAKER_00]: Yeah, I think we might get to see some in
the hyperspectral, too.
[SPEAKER_00]: But we're hoping to see the stress mostly
in the thermal.
[SPEAKER_00]: We are planning for about once every four
weeks.
[SPEAKER_00]: We're also on the hook to show that we
could operate more than one time a day,
[SPEAKER_00]: do full coverage of a fairly large field,
like a five hectare field, twice per day.
[SPEAKER_00]: So that's more of a demonstrate to ARPA-E
that we can do that kind of coverage.
[SPEAKER_00]: But the real field trips will be there
essentially once per month.
[SPEAKER_00]: I think at some point, it would just be so
close together that there wouldn't be much
[SPEAKER_00]: value.
[SPEAKER_00]: So that's roughly chosen.
[SPEAKER_00]: We should be able to see quite a bit of
change in four weeks.
[SPEAKER_00]: But I think later in maybe the third
season, we're hoping to just keep the
[SPEAKER_00]: system, keep the aircraft down there at
the field.
[SPEAKER_00]: And it could be operated more frequently
than that.
[SPEAKER_00]: So yeah, we could experiment some to see
if there's more utility and more frequent
[SPEAKER_00]: trips.
[SPEAKER_00]: Yeah, completely.
[SPEAKER_00]: I think I'm hoping to talk more about some
of the soil potentials.
[SPEAKER_00]: It'd be nice to hear some advice from you
on that.
[SPEAKER_00]: I think especially when it's large areas
to survey, the aerial approach seems
[SPEAKER_00]: really good for that.
[SPEAKER_00]: And yeah, so I think there's a good
potential there.
[SPEAKER_00]: And then we've had some first
conversations with some people about some
[SPEAKER_00]: of the pests, pest management.
[SPEAKER_00]: So yeah, I think it could be.
[SPEAKER_00]: I think a lot of ways we'd maybe be
observing.
[SPEAKER_00]: I think we've seen on the ground,
actually the ground vehicle is actually
[SPEAKER_00]: seeing the aphids and the images and
actually picking out things, able to
[SPEAKER_00]: recognize them.
[SPEAKER_00]: But from the aerial point of view,
we'd probably see more of the effect on
[SPEAKER_00]: the plant, which would be slightly
indirect but still, I think, valuable.
[SPEAKER_00]: We haven't actually been considering it in
depth yet.
[SPEAKER_00]: I think there's good potential for it.
[SPEAKER_00]: But I think just to see changes and seeing
motion of soil, the 3D data is so good
[SPEAKER_00]: from our LIDAR that I think if you're able
to see any redistributions of soil,
[SPEAKER_00]: I think that could be really good for
picking that out over time.
[SPEAKER_00]: We've done stuff where you can see small
changes in a scene just from one flight to
[SPEAKER_00]: the next, that kind of thing.
[SPEAKER_00]: But then also with picking out things like
color and thermal and hyperspectral,
[SPEAKER_00]: we might be able to see other processes
that are going on in the soils.
[SPEAKER_00]: So yeah, it'd be nice to hear advice.
[SPEAKER_04]: That's a
[SPEAKER_00]: good question.
[SPEAKER_00]: Yeah, you're right.
[SPEAKER_00]: We've been focusing on just this one,
all these different varieties of sorghum.
[SPEAKER_00]: Our focus has been more about the kind of
basic traits.
[SPEAKER_00]: But I think actually Steve and I were
talking this morning a little bit about
[SPEAKER_00]: trying to differentiate crops from weeds.
[SPEAKER_00]: And so I think we were talking about
different methods for that.
[SPEAKER_00]: I think from this high resolution camera,
just visible data, we could get at some of
[SPEAKER_00]: the geometric properties.
[SPEAKER_00]: Say some characteristics are pretty
reliable, like the shape of the leaf or
[SPEAKER_00]: the way the leaves are branching.
[SPEAKER_00]: That sort of thing could be a pretty
reliable way to differentiate between
[SPEAKER_00]: plant types.
[SPEAKER_00]: And then also I think the spectral
signatures could be potential on that.
[SPEAKER_00]: So where just something, a pretty good
correlation.
[SPEAKER_00]: Say something has some kind of spectral
signature in the infrared, maybe in the
[SPEAKER_00]: near infrared, that say corn to sorghum is
very different.
[SPEAKER_00]: So yeah, it's something we haven't done.
[SPEAKER_00]: But I bet these types of sensors could
maybe do it though.
[SPEAKER_00]: Thanks.
[SPEAKER_00]: So I guess saying once, that's good
advice.
[SPEAKER_00]: I think we've been going to the site once
per month.
[SPEAKER_00]: But like say this week, for instance,
we had some flights kind of late afternoon
[SPEAKER_00]: on one day.
[SPEAKER_00]: And then kind of earlier morning,
I guess it was more midday the next day.
[SPEAKER_00]: So temperatures and lighting conditions
were different.
[SPEAKER_00]: But still, the state of the crops is
essentially the same.
[SPEAKER_00]: So yeah, you're right.
[SPEAKER_00]: That's something that would be probably
good practice.
[SPEAKER_00]: Yeah?
[SPEAKER_01]: Thinking about commercialization for
grower
[SPEAKER_01]: wear, or selling the hardware,
and maybe have any idea what would be the
[SPEAKER_01]: default for somebody to do that later on.
[SPEAKER_00]: All right, sure.
[SPEAKER_00]: Yeah, well we're actually trying to work
on three different business models really,
[SPEAKER_00]: from our company's point of view.
[SPEAKER_00]: I think licensing the software is probably
the quickest and easiest way to market.
[SPEAKER_00]: But selling an integrated hardware
package, where it's the whole thing,
[SPEAKER_00]: basically the aircraft and the payload,
or selling just the sensor payload,
[SPEAKER_00]: those are very viable too.
[SPEAKER_00]: I think it's really hard to get much
revenue from, or to get much profit,
[SPEAKER_00]: I guess, from just selling hardware
packages really.
[SPEAKER_00]: But selling it more as a service I think
could be actually better in a lot of ways.
[SPEAKER_00]: Because a lot of the end users don't
really necessarily want to get into
[SPEAKER_00]: operating these things, or dealing with
owning them, maintaining them,
[SPEAKER_00]: and all that.
[SPEAKER_00]: But they like the data.
[SPEAKER_00]: So just for them to hire a service to come
in and collect the data and give them the
[SPEAKER_00]: data is probably a really nice model.
[SPEAKER_00]: I think in that case, our company
wouldn't, we would be the technology
[SPEAKER_00]: developers and owners of the intellectual
property.
[SPEAKER_00]: But another service providing firm would
probably do that.
[SPEAKER_00]: So those are probably the licensing and
the servicing, probably the two models
[SPEAKER_00]: that are probably the most likely to work
out well.
[SPEAKER_00]: And then as far as costs, on the aircraft
side of things, that helicopter we're
[SPEAKER_00]: working with is something like $60,000.
[SPEAKER_00]: And then the smaller one, not really
smaller, but the hexacopter one is more of
[SPEAKER_00]: a standard product now.
[SPEAKER_00]: It's more commercially available broadly
from the DJI.
[SPEAKER_00]: That one's about $15,000 if you get for
all the hardware needed.
[SPEAKER_00]: So that's a lot more attainable.
[SPEAKER_00]: For people, and then especially growers.
[SPEAKER_00]: And then the sensor package, we're looking
to get that down from a very ambitious,
[SPEAKER_00]: the one we're working with now is
something like $75,000 for just the
[SPEAKER_00]: hardware, not the labor to build it or get
it running.
[SPEAKER_00]: But then second generation, we're going to
bring that down probably more like $40,000
[SPEAKER_00]: in hardware.
[SPEAKER_00]: And then a commercialized version of that
down even further.
[SPEAKER_00]: Maybe hopefully more like $25,000.
[SPEAKER_00]: Or maybe even lower, with numbers,
with manufacturing scale.
[SPEAKER_00]: That could go even lower.
[SPEAKER_02]: Paul has told me that he does maybe have a
few slots open on his afternoon schedule.
[SPEAKER_02]: So if some of you would
[SPEAKER_02]: like to maybe schedule a lot of things,
is that time available?
[SPEAKER_00]: Yeah, that'd be great.
[SPEAKER_00]: Any advice or questions?
[SPEAKER_00]: That'd be great.
[SPEAKER_02]: And we really appreciate your coming.
[SPEAKER_02]: Thank you once again.
[SPEAKER_02]: Thank you.
[SPEAKER_04]: This has been a production of Cornell
University on the web at cornell.edu.
[SPEAKER_04]: Thank you.
Thank you.
Thank you.
