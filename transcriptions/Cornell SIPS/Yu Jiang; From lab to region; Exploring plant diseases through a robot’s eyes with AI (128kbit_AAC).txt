[SPEAKER_03]: This is a production of Cornell
University.
[SPEAKER_00]: So when I kind of got the invitation last
year from Gideon, I was super honored,
[SPEAKER_00]: but had to cancel at the last minute
because of some unexpected health issues.
[SPEAKER_00]: So we still kind of go through that
process.
[SPEAKER_00]: But then when I kind of prepare for the
seminar, I think it's not only just a
[SPEAKER_00]: seminar from the research point of view.
[SPEAKER_00]: It's actually a pretty interesting and fun
way to revisit my journey.
[SPEAKER_00]: Since joined Cornell, because believe it
or not, actually the first official
[SPEAKER_00]: Cornell seminar I got invited was from
Pompass, but dedicated in Geneva.
[SPEAKER_00]: And that's 2020.
[SPEAKER_00]: Kitty mentioned, you know, we just started
the lab.
[SPEAKER_00]: And that's the two students I have.
[SPEAKER_00]: One just graduated last fall, actually
started his own startup on commercializing
[SPEAKER_00]: the robot he developed.
[SPEAKER_00]: And second PhD is still candidate.
[SPEAKER_00]: Kind of in the pipeline, and hopefully can
graduate soon, sometime this fall.
[SPEAKER_00]: And then, you know, 22 is really kind of a
booming stage for my lab.
[SPEAKER_00]: And you can see from three kind of people
there looking at each other, most of the
[SPEAKER_00]: time just through Zoom.
[SPEAKER_00]: Now we really have all of these folks and
enjoy the in-person activities and start
[SPEAKER_00]: to do the synergy as roboticist addressing
many of these agriculture and food related
[SPEAKER_00]: issues.
[SPEAKER_00]: And last year, we actually first time had
this cross-level lab meeting involving
[SPEAKER_00]: with Kitty, Lance, and also Changchen,
who is also a new hire actually in food
[SPEAKER_00]: science, dedicated for technology for
post-harvesting evaluation.
[SPEAKER_00]: So with that, I still want kind of a
revisit.
[SPEAKER_00]: That's actually the drop talk I gave at
Cornell back to 2019.
[SPEAKER_00]: And I still think this is the kind of the
key vision.
[SPEAKER_00]: We now not only see that on the paper,
actually we got to go through that through
[SPEAKER_00]: many of the true products now working on
daily basis and support different
[SPEAKER_00]: programs, especially in plant pathology.
[SPEAKER_00]: But the basic kind of a concept is we
really want to kind of highlight using
[SPEAKER_00]: technology as a middle layer to better
bridge between the human and the
[SPEAKER_00]: agricultural systems.
[SPEAKER_00]: Instead of using tons of our wisdom and
knowledge, and we can have a technology
[SPEAKER_00]: layer to really more quantitatively gather
this knowledge.
[SPEAKER_00]: And hopefully can reserve them for many of
the future generations in a different way
[SPEAKER_00]: than just the writing books, right?
[SPEAKER_00]: And one of the good example nowadays is
like we have all these AI systems,
[SPEAKER_00]: especially large language models,
and how those can be used as a way to
[SPEAKER_00]: teach students way beyond Cornell.
[SPEAKER_00]: In my opinion, that's really kind of a
passion as a technologies I can foresee
[SPEAKER_00]: for the societal benefits.
[SPEAKER_00]: And then our kind of start really started
with the food chain solution, right?
[SPEAKER_00]: That's we call it a systems engineer.
[SPEAKER_00]: It's not a specific angle, but to look at
the whole system and try to find,
[SPEAKER_00]: oh, what other weakness is there,
right?
[SPEAKER_00]: Or the aspects to be improved.
[SPEAKER_00]: And instead of putting a lot of resources,
just solving a single problem.
[SPEAKER_00]: Perhaps for each of these aspects,
we make 1% better.
[SPEAKER_00]: Collectively, they're going to give us not
5%, but maybe 50% improvements down the
[SPEAKER_00]: road.
[SPEAKER_00]: And that's really the vision.
[SPEAKER_00]: And we kind of used the so-called
multi-modality, multi-scale sensing
[SPEAKER_00]: technology.
[SPEAKER_00]: It's also not just a ground robots where
we're going to focus a lot today,
[SPEAKER_00]: but we have in collaboration with Lance,
we have the Backbird as more like a
[SPEAKER_00]: laboratory microscopic imaging robot that
we can gather the information really from
[SPEAKER_00]: the wet lab experiments.
[SPEAKER_00]: And then translated to the ground robots
to get sub-millimeter, very high
[SPEAKER_00]: resolution image and information from the
canopies of the plants in the field and
[SPEAKER_00]: all the way to the drone or UAS-based
imagery to get what the field looks like,
[SPEAKER_00]: right?
[SPEAKER_00]: Or what's this kind of a whole vineyard or
orchard looks like all the way to how the
[SPEAKER_00]: New York state and the global look like
through the satellite remote sensing
[SPEAKER_00]: technology.
[SPEAKER_00]: And hopefully, we can also gather all this
knowledge through from molecular all the
[SPEAKER_00]: way to the global as well.
[SPEAKER_00]: And in my opinion, that's where AI really,
really helpful because we, I mean,
[SPEAKER_00]: as human, we have limited power to really
figure out very complex relationships.
[SPEAKER_00]: And that's really these computational
tools can help us in the long run.
[SPEAKER_00]: And then when I kind of look back,
all right, so we talk about from all the
[SPEAKER_00]: way from breeding to pre-season,
in-season, post-harvest, harvest,
[SPEAKER_00]: post-harvest, right?
[SPEAKER_00]: When I just joined, all the bottom part
are blank.
[SPEAKER_00]: Okay, so nothing there, it's just a blank
page and we start filling.
[SPEAKER_00]: And in the past four years around,
we kind of fill in most of these major
[SPEAKER_00]: components.
[SPEAKER_00]: As I mentioned, in collaboration with
Lance and in the early stage also,
[SPEAKER_00]: David Goddary, also from plant pathology
at Geneva, we have this microscopic
[SPEAKER_00]: imaging robot and we actually just
submitted a new initiative for USDA AR
[SPEAKER_00]: because the ARS kind of leadership is very
interested in pushing this technology for
[SPEAKER_00]: all ARS labs across the country to do the
high throughput phenotyping to better
[SPEAKER_00]: understand the plant's responses to
different stress stimulations.
[SPEAKER_00]: And in the middle, that's where we do for
not only the breeding, like we have a
[SPEAKER_00]: field of trials, the programs to evaluate
their actual field responses, right,
[SPEAKER_00]: to different diseases, but we also have
the drone systems that really can be
[SPEAKER_00]: leveraged for faster and kind of
equivalently accurate sensing capacity to
[SPEAKER_00]: the ground system.
[SPEAKER_00]: And the last piece actually is this new
lab scanner.
[SPEAKER_00]: Gideon and I really work a lot from the
breeding, right, all the way to how we
[SPEAKER_00]: manage the grapevines in the field during
the season.
[SPEAKER_00]: And this is the last piece is,
okay, we got disease infection for
[SPEAKER_00]: whatever reason and the measurement that
does not really take effectiveness.
[SPEAKER_00]: And we ended up with all these infected
grape berries and how, at the last step,
[SPEAKER_00]: people can evaluate their defectiveness
and really sort out the highest quality
[SPEAKER_00]: that the growers still can gain the
monetary benefits rather than,
[SPEAKER_00]: okay, this is a kind of a totally died one
and we have no idea how to make money from
[SPEAKER_00]: that.
[SPEAKER_00]: And we just kind of published this paper
last year and hopefully to just continue
[SPEAKER_00]: in collaboration with Terry Bates from
Claro at Western New York.
[SPEAKER_00]: So that's the kind of a major thing we
have achieved.
[SPEAKER_00]: But when I kind of think about what's the
vision, right, for a roboticist to work
[SPEAKER_00]: with plant pathology in a broader context,
this is the kind of image I generated and
[SPEAKER_00]: I use, I'm a very passionate roboticist,
interested in plant pathology question.
[SPEAKER_00]: And my dream is to help my colleague
really dealing with many of the daily
[SPEAKER_00]: scouting issues.
[SPEAKER_00]: Please help me generate a figure to
demonstrate how this can be more
[SPEAKER_00]: collaborative, period.
[SPEAKER_00]: That's the text prompt.
[SPEAKER_00]: It's very long, but then just give me this
very beautiful image, right, and that's
[SPEAKER_00]: exactly what I envisioned.
[SPEAKER_00]: We still need a lot of human pathologies
in the room to use our kind of natural
[SPEAKER_00]: intelligence to advance the knowledge,
really help people to understand this
[SPEAKER_00]: knowledge.
[SPEAKER_00]: But on the other hand, we start to have
the robots to assist you as a human
[SPEAKER_00]: scientist.
[SPEAKER_00]: Again, to better understand all these
interactions and hopefully to also
[SPEAKER_00]: identify insights or any of the complex
relationships otherwise human,
[SPEAKER_00]: ourself cannot easily identify,
right.
[SPEAKER_00]: So when we kind of look at a two line of
numbers, try to do regression,
[SPEAKER_00]: yes, we can even manually draw the scatter
plot and try to identify some relationship
[SPEAKER_00]: there, not a matter that's linear or
nonlinear.
[SPEAKER_00]: But when you have thousand dimensions of
the data, the first challenge to me is
[SPEAKER_00]: like, okay, I stop working.
[SPEAKER_00]: The dimensionality is just booming and how
this AI and robotics technology can help
[SPEAKER_00]: us.
[SPEAKER_00]: And then that's another kind of a key
thing.
[SPEAKER_00]: I did not really kind of see, Bassey
actually would be here, but I'm so
[SPEAKER_00]: appreciated you come.
[SPEAKER_00]: Because that's kind of relevant also,
in my opinion, to a very traditional field
[SPEAKER_00]: in Cornell called agricultural
engineering.
[SPEAKER_00]: I think many of you who are senior really
know that department used to be actually
[SPEAKER_00]: the leader, not only in the country,
but worldwide, to push for all the
[SPEAKER_00]: agricultural mechanization.
[SPEAKER_00]: But somehow we kind of shift the entries
and now we're catching up for the digital
[SPEAKER_00]: agriculture era.
[SPEAKER_00]: And one of the key things or challenge for
all the agricultural engineers,
[SPEAKER_00]: in my opinion, is we lack actually a solid
science foundation to start with.
[SPEAKER_00]: Okay, the analog I kind of try to compare
is we consider biomedical engineering
[SPEAKER_00]: versus agriculture engineering.
[SPEAKER_00]: What's the key difference there?
[SPEAKER_00]: Only the study object is different.
[SPEAKER_00]: One is for human health, the other is for
plant health, for plant pathology context,
[SPEAKER_00]: right?
[SPEAKER_00]: Other than that, it's just similar.
[SPEAKER_00]: And actually plants give us more
advantages.
[SPEAKER_00]: Because we can create more samples,
more replicates, different genetic
[SPEAKER_00]: sources, and we can leverage all these to
really advance our understanding about the
[SPEAKER_00]: life of mechanisms.
[SPEAKER_00]: In addition to just help us to manage the
human disease.
[SPEAKER_00]: Which is also very important, but it's
just like we have a lot of other ethical
[SPEAKER_00]: issues that we don't want to kind of risk
ourselves, right?
[SPEAKER_00]: So plants actually is very advantageous.
[SPEAKER_00]: But oftentimes the key thing is like,
I don't know where I can start with from a
[SPEAKER_00]: science point.
[SPEAKER_00]: Do I have a solid science framework to
guide my engineering advancement?
[SPEAKER_00]: And hopefully use those advanced
engineering technologies to better
[SPEAKER_00]: understand the science as a closed loop.
[SPEAKER_00]: So Gideon and I kind of chatted a lot over
the COVID.
[SPEAKER_00]: And this is really the figure I want to
borrow from her.
[SPEAKER_00]: Because this is a kind of a very cool
starting point as an engineer to find the
[SPEAKER_00]: science foundation.
[SPEAKER_00]: When we talk about the disease,
many of you, I believe, know more than me.
[SPEAKER_00]: We have the host, we have the pathogen,
and then we have the environment.
[SPEAKER_00]: And somehow because of her program
entries, we added the management
[SPEAKER_00]: explicitly.
[SPEAKER_00]: But I still consider that as a part of the
environment from the engineering
[SPEAKER_00]: perspective.
[SPEAKER_00]: Sorry, Katie.
[SPEAKER_00]: So we started with, OK, let's just pick
these three.
[SPEAKER_00]: Pathogen, that's a kind of a different
word in my opinion.
[SPEAKER_00]: And we're going to just put that aside a
little bit.
[SPEAKER_00]: And we're going to find some hot time
later on.
[SPEAKER_00]: So this is actually the starting point.
[SPEAKER_00]: 2019, December, I heard Cornell has no
tradition to close the campus because of
[SPEAKER_00]: the snow.
[SPEAKER_00]: But my first day was a heavy snow and the
campus was closed.
[SPEAKER_00]: I've been told your first day will be
holiday.
[SPEAKER_00]: Thank you, Cornell.
[SPEAKER_00]: So that's actually the system.
[SPEAKER_00]: So my colleagues already used in the field
to collect a lot of images and try to
[SPEAKER_00]: better understand what's the disease
resistance response in the field.
[SPEAKER_00]: And then we start to come up with a more
automated kind of a concept on the right
[SPEAKER_00]: hand side.
[SPEAKER_00]: And you will be laughing, oh, what that
small kind of a toy robot can do.
[SPEAKER_00]: But that's our starting point.
[SPEAKER_00]: We start to add the GPS, add different
sensors, and start to automate the data
[SPEAKER_00]: acquisition process.
[SPEAKER_00]: But what is really key is two components
there.
[SPEAKER_00]: When you talk about autonomous field
scouting, we need to actually solve two
[SPEAKER_00]: problems.
[SPEAKER_00]: Number one, we need a navigation system.
[SPEAKER_00]: Just as the Tesla that now can drive just
on the road, we need the robot to just
[SPEAKER_00]: traverse the whole backyard or all chart
or field autonomously without any human
intervention.
[SPEAKER_00]: And the second component is how the
disease will be sensed from the images we
[SPEAKER_00]: collected.
[SPEAKER_00]: So that if we put these two together,
OK, yeah, autonomous disease scouting,
[SPEAKER_00]: at least as a prototype.
[SPEAKER_00]: So this is the published work.
[SPEAKER_00]: And here we actually use a pure vision
based guidance.
[SPEAKER_00]: On the left side, that's the input image
seen by the camera from the front view.
[SPEAKER_00]: And then the right side is actually the
AI-based determined pathway.
[SPEAKER_00]: The robot should go forward to traverse
the whole backyard without any human
[SPEAKER_00]: interference.
[SPEAKER_00]: And surprisingly, we tested that out in
different field conditions.
[SPEAKER_00]: It generated very well.
[SPEAKER_00]: So we collected the data, actually,
in one of Kitty's vineyards in AgriTech.
[SPEAKER_00]: And we tested out that during the winter
time in California, different vineyards in
[SPEAKER_00]: New York.
[SPEAKER_00]: And they just all worked pretty nicely.
[SPEAKER_00]: So that gave us the confidence,
all right, we have the navigation capacity
[SPEAKER_00]: here.
[SPEAKER_00]: And then we built the neural network,
which is a buzzword nowadays, and to
[SPEAKER_00]: really identify where these disease
infection regions are on the images.
[SPEAKER_00]: And we also further optimized the neural
network so that we can get it faster and
[SPEAKER_00]: can be deployed on the robot directly.
[SPEAKER_00]: And putting these two together,
that's, I believe, well, sure,
[SPEAKER_00]: why?
[SPEAKER_00]: Many of you have seen this before.
[SPEAKER_00]: While the robot is traversing through the
vineyard, we got the input image.
[SPEAKER_00]: We got where the disease are.
[SPEAKER_00]: We got the canopies are.
[SPEAKER_00]: And then we also avoid any of the
overlapping between consecutive images,
[SPEAKER_00]: right?
[SPEAKER_00]: Because we don't want double count
anything there.
[SPEAKER_00]: And if we aggregate them all together,
we can get the actual disease infection
[SPEAKER_00]: severity.
[SPEAKER_00]: And the good thing is, once we have this
robot, right, it's no longer, oh,
[SPEAKER_00]: as Kitty mentioned, we need to have a
group of five or six.
[SPEAKER_00]: Send them all to the field.
[SPEAKER_00]: Folks, let's do the scouting.
[SPEAKER_00]: And after a week, we probably finished
some kind of field scouting.
[SPEAKER_00]: And we know time is important for plant
pathology, because disease is not a static
[SPEAKER_00]: thing.
[SPEAKER_00]: They keep developing themself,
so there are a lot of variants there.
[SPEAKER_00]: So what we found is, actually,
the scouting frequency is really a key,
[SPEAKER_00]: right?
[SPEAKER_00]: So we deploy this robot based on a weekly
basis.
[SPEAKER_00]: And if we want, we can actually deploy
that on a daily basis.
[SPEAKER_00]: And the interesting thing is, we
oftentimes talk about asymptomatic disease
[SPEAKER_00]: detection as the key information for the
growers to know, OK, I need to apply
[SPEAKER_00]: preventive measures to suppress this
disease.
[SPEAKER_00]: This is pressure, right, at the good
starting point.
[SPEAKER_00]: And oftentimes, asymptomatic disease
detection is super challenging.
[SPEAKER_00]: And that's really because we have limited
manpower to scout that frequently.
[SPEAKER_00]: And now with this robot can autonomously
scout the vineyards on a weekly basis,
[SPEAKER_00]: what we found is, at the beginning,
that's the map, the robot gap,
[SPEAKER_00]: if you can see my cursor here.
[SPEAKER_00]: And you see some of these purple-ish
blocks.
[SPEAKER_00]: That's actually the panels start to show
very, very early symptoms.
[SPEAKER_00]: But if you look at the whole vineyard,
it seems like that's fine.
[SPEAKER_00]: It won't really trigger any of the serious
consideration.
[SPEAKER_00]: But when you kind of later on,
all the way, waited for three weeks,
[SPEAKER_00]: the human scouting really confirmed,
no, you've got serious disease issue.
[SPEAKER_00]: And this time window in between,
is what this robot can really fill in,
[SPEAKER_00]: right?
[SPEAKER_00]: Because we don't have the limitation
because of the manpower.
[SPEAKER_00]: You just say, you deploy this robot,
and you detect this map.
[SPEAKER_00]: And as early as just a week, we'll already
identify these hotspots.
[SPEAKER_00]: And hopefully, if we come up with the
right measurement strategy, we can contain
[SPEAKER_00]: them well and avoid getting into this
situation.
[SPEAKER_00]: So with that, what we kind of want to come
away is, let's kind of explore further.
[SPEAKER_00]: What about the frequency of scouting can
impact how the disease management
[SPEAKER_00]: practices can be?
[SPEAKER_00]: In the past, we have to wait for the
scouting consultants coming in and tell us
[SPEAKER_00]: what happened.
[SPEAKER_00]: But now with this robot, if I do
frequently, let's see push for even daily
[SPEAKER_00]: basis.
[SPEAKER_00]: What that can tell us was the new
information we can leverage there.
[SPEAKER_00]: So the other thing I want to point out is
actually the improved scouting
[SPEAKER_00]: sensitivity.
[SPEAKER_00]: So this is a kind of a commercial vineyard
in Finger Lakes region.
[SPEAKER_00]: And we are targeting to detect the grape
leaf rotavirus, which has a very obvious
[SPEAKER_00]: visual feature.
[SPEAKER_00]: It's like a radish infection on a green
leaf.
[SPEAKER_00]: So that's very kind of need for this robot
to do.
[SPEAKER_00]: And what we found is, in general,
the robot and the human scouting align
[SPEAKER_00]: very well, like over 90% accuracy.
[SPEAKER_00]: But for some of the spots, the robot
reported, hey, here, if you see this dot,
[SPEAKER_00]: we got some diseases there.
[SPEAKER_00]: But human report none.
[SPEAKER_00]: What interesting, oh, I know the model is
not perfect.
[SPEAKER_00]: And we all know model won't be perfect.
[SPEAKER_00]: But some works.
[SPEAKER_00]: And then the good thing is, first,
our impression is not to check all the lab
[SPEAKER_00]: nodes.
[SPEAKER_00]: It's to go back, check our database,
really pull out what's the GPS information
[SPEAKER_00]: for that particular region.
[SPEAKER_00]: And then what's the corresponding raw
image look like?
[SPEAKER_00]: And that's exactly what we show on the
left side.
[SPEAKER_00]: That's a raw image.
[SPEAKER_00]: And then we have this model inference
result on the right.
[SPEAKER_00]: And we identify, oh, there is a very,
very small portion being labeled as GRRV
[SPEAKER_00]: infection as here.
[SPEAKER_00]: And I hope the contrast is enough.
[SPEAKER_00]: But we kind of zoom in this region and
really bring to people, and we confirm.
[SPEAKER_00]: Yes, that's a reddish symptoms on green
leaf.
[SPEAKER_00]: So we confirm there is a GRRV or kind of a
GRRV infection symptom there.
[SPEAKER_00]: So that really give us the confidence,
not only about the accuracy, absolute
[SPEAKER_00]: accuracy, but also the way how we think
and how we retrieve the information that
[SPEAKER_00]: we think might be a surprise to our
research hypotheses.
[SPEAKER_00]: And that's where this robot can do now.
[SPEAKER_00]: So then we kind of distribute the PPB into
different regions.
[SPEAKER_00]: And this is supported, actually,
by the SCRI grant called the VITES Gen 3.
[SPEAKER_00]: It's the third generation.
[SPEAKER_00]: And we have collaborators all the way from
California, South Dakota, Wisconsin,
[SPEAKER_00]: and now Wisconsin, Minnesota, Western
Virginia, and, of course, New York.
[SPEAKER_00]: We have a kiddies group program.
[SPEAKER_00]: We have a maddies group program on grid
braiding.
[SPEAKER_00]: And also last program from the ARS
perspective.
[SPEAKER_00]: And then what we kind of wondering is,
OK, so we have these good tools for our
[SPEAKER_00]: researchers.
[SPEAKER_00]: And many of them can afford that for sure.
[SPEAKER_00]: And we don't need to worry.
[SPEAKER_00]: I mean, the only thing we need to worry is
when we're going to start to get these
[SPEAKER_00]: agencies back to function normally.
[SPEAKER_00]: And we can write grant proposals and start
to acquire these tools for our research,
[SPEAKER_00]: routine research.
[SPEAKER_00]: But for growers, the challenge is always
being around the cost and also their
[SPEAKER_00]: return on investment.
[SPEAKER_00]: And for each of the PPB prototype,
roughly just the building itself costs
[SPEAKER_00]: $35,000.
[SPEAKER_00]: And if you consider for an entrepreneur to
survive, you need to at least double that.
[SPEAKER_00]: So each vendor need to pay for $70,000 to
get one robot.
[SPEAKER_00]: I'm not sure when they can justify the
ROI.
[SPEAKER_00]: But of course, I don't want to be the one
to create further disparity because of the
[SPEAKER_00]: technology investment.
[SPEAKER_00]: That's not how we've been passionate about
technology investment.
[SPEAKER_00]: So that's the reason we just do some kind
of a brief comparison among the vendors in
[SPEAKER_00]: different states.
[SPEAKER_00]: And we found, OK, remote sensing is the
only viable option.
[SPEAKER_00]: Because we have the federal agencies like
NASA.
[SPEAKER_00]: And they have a lot of resources to offer
affordable or even free satellite imagery
[SPEAKER_00]: that people can use.
[SPEAKER_00]: And also, we have some other private
sectors that launch commercial satellites
[SPEAKER_00]: that will ultimately enable such a thing
happening using remote sensing technology.
[SPEAKER_00]: But the question is how, right?
[SPEAKER_00]: So when you consider the current remote
sensing kind of work, largely it is,
[SPEAKER_00]: on one hand, we need to gather data from
the air.
[SPEAKER_00]: And on the other hand, we need to have the
so-called ground troops so that they can
[SPEAKER_00]: really build a model to correlate the
signals from your remote sensing data set
[SPEAKER_00]: versus what we observed from the field.
[SPEAKER_00]: And later on, we can translate or dispatch
this model for the actual uses.
[SPEAKER_00]: But oftentimes, getting the remote sensing
data is no longer being the challenge,
[SPEAKER_00]: at least with all this engineering
advancement.
[SPEAKER_00]: But who will be the human power get the
ground troops measurements from the field,
[SPEAKER_00]: right?
[SPEAKER_00]: You all run your field experiments.
[SPEAKER_00]: And I, being very lucky, have my
colleagues like Katie to run that.
[SPEAKER_00]: I know they have a group of folks just
dedicated for ground trucing.
[SPEAKER_00]: It's very tedious laborers.
[SPEAKER_00]: And no one really, I wouldn't say not
really enjoy it.
[SPEAKER_00]: But when you're repeatedly doing that,
then that's where borrowing starts,
[SPEAKER_00]: right?
[SPEAKER_00]: So our idea is why not we can leverage our
ground robots as a source to ground truce
[SPEAKER_00]: what happening on the ground and feed this
ground truce data back to the satellite so
[SPEAKER_00]: that we can build a model between the
robot-generated ground truce versus the
[SPEAKER_00]: satellite imagery so that we can have a
model but with a much affordable cost or
[SPEAKER_00]: press tag to build it.
[SPEAKER_00]: So here is what we kind of tried actually
last season.
[SPEAKER_00]: First, we deploy the robots in the vane
area.
[SPEAKER_00]: And as we demonstrated there, we can
identify different disease and generate a
[SPEAKER_00]: geospatial map with what's their severity
there.
[SPEAKER_00]: And instead of just having human ground
trucing, we purely rely on this map as the
[SPEAKER_00]: ground trucing because we know the
accuracy is pretty high.
[SPEAKER_00]: And we can confidently trust it.
[SPEAKER_00]: And then we just collect the drone data or
aircraft hyperspectral data actually later
[SPEAKER_00]: on.
[SPEAKER_00]: And then we build a model, try to see
whether or not we can use the
[SPEAKER_00]: robot-generated ground truce to train a
model to interpret the UAV or remote
[SPEAKER_00]: sensing imagery into disease severity.
[SPEAKER_00]: And here is what we got.
[SPEAKER_00]: The first case is actually, again,
research minority in every tech.
[SPEAKER_00]: And we use the UAV, which has much higher
spatial resolution, but we use
[SPEAKER_00]: multispectral imaging, which we have less
spectral information.
[SPEAKER_00]: And what we found is actually when the
kind of a disease approaching to its peak
[SPEAKER_00]: time or end time, the accuracy or the
correlation between the robot-generated
[SPEAKER_00]: ground truce versus the remote sensing or
UAV signal is really high.
[SPEAKER_00]: And we can push for a model of accuracy
about 80%.
[SPEAKER_00]: That would be really useful because you
don't need to really rely on very
[SPEAKER_00]: time-consuming human scouting in the
field.
[SPEAKER_00]: You can actually borrow this model and
then just fly the drone with the hope we
[SPEAKER_00]: can deliver 80% accuracy, quote unquote,
so that the growers can really use a
[SPEAKER_00]: faster speed to know what is happening in
the field.
[SPEAKER_00]: But we all know what is not really good
for multispectral imaging is it lacks of
[SPEAKER_00]: certain spectral resolution that can
really resolve the signal relevant to the
[SPEAKER_00]: disease infection or the disease itself.
[SPEAKER_00]: So that's the reason we leverage the
hyperspectral technology.
[SPEAKER_00]: But before we kind of getting into build a
model, what we do is first, as a data
[SPEAKER_00]: person, I really want to have a very solid
statistic evidence to show, yes,
[SPEAKER_00]: that based on the data intrinsics,
they are separable.
[SPEAKER_00]: So what do we do?
[SPEAKER_00]: Is we use both human field scouting as a
ground truce and also robot field scouting
[SPEAKER_00]: as a ground truce.
[SPEAKER_00]: And then we label all the pixels from the
remote sensing data set and try to see
[SPEAKER_00]: what's the separation power between
healthy and infected plants.
[SPEAKER_00]: And what we found is actually the robot is
not only on par, but actually give a
[SPEAKER_00]: little bit more statistical power.
[SPEAKER_00]: In generating the separation between
healthy and infected plants through the
[SPEAKER_00]: hyperspectral data.
[SPEAKER_00]: To me, that's a really kind of exciting
thing because then we have the solid
[SPEAKER_00]: foundation to build the model.
[SPEAKER_00]: And here is just a highlight of that
particular region.
[SPEAKER_00]: And you can see for the human,
the mean to mean is almost very close to
[SPEAKER_00]: each other.
[SPEAKER_00]: And if you consider the standard
deviation, that just created a challenge
[SPEAKER_00]: for the modeling itself.
[SPEAKER_00]: But when you look at the bottom part,
that's where the ground truce is from the
[SPEAKER_00]: robot.
[SPEAKER_00]: You start to see clear kind of a
separation even between the two means.
[SPEAKER_00]: So that give us additional power to
believe the model should get improved.
[SPEAKER_00]: So with that, we just build the model.
[SPEAKER_00]: And of course, we kind of test it out on
that hyperspectral imagery collected by
[SPEAKER_00]: the aircraft.
[SPEAKER_00]: And 5% improvement.
[SPEAKER_00]: So we are still kind of waiting for more
data from the coming season and try to see
[SPEAKER_00]: if this is a solid 5% improvement or it
could be just a variation by nature.
[SPEAKER_00]: And it is hard to tell at this point.
[SPEAKER_00]: But at least give us a very promising
results from the data analytics
[SPEAKER_00]: perspective.
[SPEAKER_00]: And of course, with that, as we kind of
discussed, what's the motivation there?
[SPEAKER_00]: It's really to have a scalable solution
that everyone can afford to use advanced
[SPEAKER_00]: technologies.
[SPEAKER_00]: And that's the comparison.
[SPEAKER_00]: Very easy.
[SPEAKER_00]: A group of 45 people do the human scouting
for this venue.
[SPEAKER_00]: That's actually a venue out in California.
[SPEAKER_00]: It takes a week.
[SPEAKER_00]: And with the robot, we need several hours
to run the robots autonomously there.
[SPEAKER_00]: I mean, although you don't need to think,
oh, I don't need to traverse there,
[SPEAKER_00]: how much power the robot can consume,
based on the current regulatory in
[SPEAKER_00]: California and actually most of the states
in the US, you need to follow the
[SPEAKER_00]: self-driven car traffic policy for having
autonomous robot running in your field.
[SPEAKER_00]: I do not understand about that part,
but that's what we've been regulating.
[SPEAKER_00]: So I have brought that up to actually many
of the New York legislators.
[SPEAKER_00]: And hopefully, they start to discuss what
would be the policy from New York to
[SPEAKER_00]: really embrace this technology and being
ahead of other states or even the world
[SPEAKER_00]: about these new regulators for autonomous
equipment in agriculture.
[SPEAKER_00]: And lastly is actually the NASA average
data acquisition, just the whole region.
[SPEAKER_00]: And then it takes several minutes to run
the model.
[SPEAKER_00]: Yes, we got it.
[SPEAKER_00]: OK, so that's really the difference in
terms of how soon we can get and how large
[SPEAKER_00]: the region we can cover in terms of using
remote sensing plus with all these robot
[SPEAKER_00]: technologies together.
[SPEAKER_00]: So up to this point, I would say,
OK, we have the autonomous disease
[SPEAKER_00]: scouting.
[SPEAKER_00]: We have the disease detection models by
supervisor learning.
[SPEAKER_00]: So if you work on machine learning,
this is the way a human needs to
[SPEAKER_00]: explicitly tell the answer what this is to
the model so the model can learn what this
is.
[SPEAKER_00]: The plan is like we need human labels.
[SPEAKER_00]: And I know many of you don't want to do
that.
[SPEAKER_00]: And then we have a proof of concept
scalable scouting from the field to the
[SPEAKER_00]: region levels through this ground to aero
branch of this closed loop analysis.
[SPEAKER_00]: But what is missing there is actually
looking at this map, that's how we
[SPEAKER_00]: distribute our robots across the country.
[SPEAKER_00]: It's actually the model generalized
capability, which is also key for many of
[SPEAKER_00]: these AI research is the model trained on
one data set, whether or not I can rely on
[SPEAKER_00]: for many of our same data sets in the
future.
[SPEAKER_00]: And what we did is, OK, instead of just
rely on the image itself, which give us
[SPEAKER_00]: the real feature, we leverage the
multimodal models.
[SPEAKER_00]: So we use the language as a regulator
because that's how many of us as a human
[SPEAKER_00]: pathologist really can communicate because
I'm learning from you.
[SPEAKER_00]: And I'm not kind of learning from you to
point all the pixels on the images saying,
[SPEAKER_00]: hey, that's the pottery molding infection.
[SPEAKER_00]: That's the Donnie molding infection.
[SPEAKER_00]: That's your army.
[SPEAKER_00]: I've been told just when you look at the
pottery molding, that's a whitish layer on
[SPEAKER_00]: top of the leaf.
[SPEAKER_00]: And you need an acute angle to look at
that so you can really see it clearly.
[SPEAKER_00]: That's how we really learn.
[SPEAKER_00]: And that's what we really want to push for
AI technology.
[SPEAKER_00]: Is why not to use text description as a
way to tell the model what is the feature
[SPEAKER_00]: looks like in the image so that combining
both the image as a visual inputs and the
[SPEAKER_00]: text as another inputs, the model actually
can get better performance.
[SPEAKER_00]: So here is just a quick thing to show.
[SPEAKER_00]: With this improvements, we got a much
higher accuracy as compared with the
[SPEAKER_00]: baseline model in the middle and also
another kind of a large foundational model
[SPEAKER_00]: called SAM.
[SPEAKER_00]: I think many of you also use that maybe.
[SPEAKER_00]: So get some improvements there.
[SPEAKER_00]: So that really demonstrate, yes,
our hypothesis is correct.
[SPEAKER_00]: And when you try to use additional
modality or the modality information,
[SPEAKER_00]: and the whole model performance will be
boosted because the model is really trying
[SPEAKER_00]: to learn some of the knowledge rather than
the superficial pixel relationships.
[SPEAKER_00]: So then what we want to test is not just
for the model, but for the whole model.
[SPEAKER_00]: So we talked about the model generalize
capability and that's our target.
[SPEAKER_00]: What about we just pick one data set from
California?
[SPEAKER_00]: And this is a truly insane data set.
[SPEAKER_00]: We have no idea what's going on.
[SPEAKER_00]: And if you look at all these images,
even using different scoring system than
[SPEAKER_00]: what we use in New York, their training
system is different.
[SPEAKER_00]: And you see each one is a very bulky
canopy and being very well separated as
[SPEAKER_00]: compared as a panel, like a whole panel of
leaves in New York, right?
[SPEAKER_00]: And how the model gonna perform?
[SPEAKER_00]: That's the first thing.
[SPEAKER_00]: We got just a zero shot, which means we
train our model only using the New York
[SPEAKER_00]: data set.
[SPEAKER_00]: Done.
[SPEAKER_00]: Modeling part is done.
[SPEAKER_00]: And we directly use that model infer the
disease infection on the California data
[SPEAKER_00]: sets.
[SPEAKER_00]: And that's what we found actually across
two years from 23 and 24.
[SPEAKER_00]: So this is the error distribution.
[SPEAKER_00]: Remember they have the scoring system from
zero to six, right?
[SPEAKER_00]: Or zero to five, six different levels.
[SPEAKER_00]: And in the middle line, that's agreement
with no scoring category difference at
[SPEAKER_00]: all.
[SPEAKER_00]: And then when you look at the overall
distribution, maximum difference,
[SPEAKER_00]: categorical difference would be just a one
level, either minus one or plus one.
[SPEAKER_00]: And this is a very consistent through two
years of this testing.
[SPEAKER_00]: And we are very happy about that.
[SPEAKER_00]: So that demonstrates, okay, these models
are generalized but what's the next?
[SPEAKER_00]: There's actually several hard times.
[SPEAKER_00]: Get ready.
[SPEAKER_00]: So the first thing we are kind of thinking
is at the beginning from the project,
[SPEAKER_00]: people are more interested in or
concerning whether or not the robot can
[SPEAKER_00]: detect very low incidence cases,
right?
[SPEAKER_00]: Because they are very challenging for
computer vision.
[SPEAKER_00]: But what we found actually from the
California data set is no, actually the
[SPEAKER_00]: model handle those low incidence cases
pretty good, I would say.
[SPEAKER_00]: But the more problematic with high
incidence, and this is understandable
[SPEAKER_00]: because for many of the breeders or even
pathologists, after certain threshold,
[SPEAKER_00]: you're gonna just say no.
[SPEAKER_00]: That's a useless thing.
[SPEAKER_00]: I'm gonna just give whatever score there,
right?
[SPEAKER_00]: Regardless that's really 55% or 75% or
95%.
[SPEAKER_00]: I all call them score five, right?
[SPEAKER_00]: So that really being the key challenge but
is actually a good thing because our model
[SPEAKER_00]: actually give better percentage
quantification rather than just a score.
[SPEAKER_00]: The second aha moment is, okay,
now we have a really disease progression,
[SPEAKER_00]: right?
[SPEAKER_00]: As I mentioned, it's not just a single
time point.
[SPEAKER_00]: We can frequently deploy that on weekly
basis and we can generate this disease
[SPEAKER_00]: progression curve.
[SPEAKER_00]: And as a data scientist, I'm lazy,
right?
[SPEAKER_00]: And I lack of certain pathology knowledge.
[SPEAKER_00]: So what I do is really from the data
perspective, that's just around the
[SPEAKER_00]: clustering algorithm and see what we can
learn from these disease progressive
[SPEAKER_00]: curves.
[SPEAKER_00]: And very surprising, we found a kind of a
cluster.
[SPEAKER_00]: If you see starting with adding four
clusters there, there is a bottom blue
[SPEAKER_00]: line showing consistently very low disease
infection symptoms there, right?
[SPEAKER_00]: So from the breeding perspective,
that's exactly the group we want to
[SPEAKER_00]: identify because they really give a very
strong resistance to the disease in the
[SPEAKER_00]: natural environment.
[SPEAKER_00]: And that's what we really want to kind of
find using big data, right?
[SPEAKER_00]: Because then breeding is a number game,
in my opinion.
[SPEAKER_00]: The more numbers we put in, the kind of
higher likelihood we're gonna get
[SPEAKER_00]: something useful.
[SPEAKER_00]: And this is really enables us to put more
and more and more numbers there and to
[SPEAKER_00]: really enhance our possibility of getting
the right materials for different
[SPEAKER_00]: environment or different desires.
[SPEAKER_00]: Moment three is actually, okay,
so we compare, right?
[SPEAKER_00]: We have this progression curve,
but how that kind of compare among
[SPEAKER_00]: different years?
[SPEAKER_00]: So what we usually kind of expect is
actually the left side, right?
[SPEAKER_00]: If this is the genotype with certain
disease-resistant genes kind of stacked
[SPEAKER_00]: there, we expect it demonstrate also high
resistance to any of the pathogens.
[SPEAKER_00]: Like for example, this is for powdery
mildew, right?
[SPEAKER_00]: But actually through the data analysis,
we found the right side case.
[SPEAKER_00]: That particular genotype also have a stack
of resistant genes, but the why in one
[SPEAKER_00]: year is behave very normal, right?
[SPEAKER_00]: If you look at the blue and the red lines,
that's actually the data from 23.
[SPEAKER_00]: But when you look at the black lines,
that's the data from 24.
[SPEAKER_00]: It got kind of infected a lot by powdery
mildew, right?
[SPEAKER_00]: And that really give us a new science
question.
[SPEAKER_00]: What happened?
[SPEAKER_00]: That's the part.
[SPEAKER_00]: Bring back this chart, right?
[SPEAKER_00]: So we discuss about all the plant
environment management, but what we lack
[SPEAKER_00]: is what's the interaction from the
pathogen point of view, right?
[SPEAKER_00]: There are so many isolate, and we know the
mutant rates is very high for those mites.
[SPEAKER_00]: And how that being considered so far in
the whole breeding cycle or whole
[SPEAKER_00]: management cycle.
[SPEAKER_00]: And now our new tools really enables us to
do that.
[SPEAKER_00]: And actually that right case, if you look
at it here, is kind of overcome by a
[SPEAKER_00]: specific isolate of powdery mildew only
identified in California, not in New York.
[SPEAKER_00]: But it might be a matter of time when it
can be here.
[SPEAKER_00]: So with that, we have some new chapters to
ongoing, right?
[SPEAKER_00]: We have affordable, scalable solution,
especially now we want to bridge the
[SPEAKER_00]: closed loop from the arrow to the ground,
right?
[SPEAKER_00]: Rubots are very neat, but they are so
expensive.
[SPEAKER_00]: And how to strategically deploy them into
different places that can help the remote
[SPEAKER_00]: sensing model even being better than
nowadays.
[SPEAKER_00]: So that's our next question, and also it's
an economic question.
[SPEAKER_00]: Now this ag-tech sector being solely
focused in California, which I understand
[SPEAKER_00]: why, but is this the right way to allocate
all the resources there?
[SPEAKER_00]: And how to strategically really expand and
reallocate these resources both help the
[SPEAKER_00]: girls to combat their production kind of
challenges, but also help these startup
[SPEAKER_00]: companies really gain economic benefits.
[SPEAKER_00]: And hopefully that can also boost regional
economy because of the technology.
[SPEAKER_00]: And then of course with this new approach,
we start to add a pathogen back in the
[SPEAKER_00]: whole thing, right?
[SPEAKER_00]: Then we really have four components there
try to identify their complex
[SPEAKER_00]: relationships and we hope to reveal more.
[SPEAKER_00]: And the last part is a participatory
research approach.
[SPEAKER_00]: We talk about this high tech, but my heart
goes with people and where we are right
[SPEAKER_00]: now.
[SPEAKER_00]: And that's actually ongoing effort from my
student supported by NASA.
[SPEAKER_00]: We call it the Pharmacology Network.
[SPEAKER_00]: And basically is if everyone, doesn't
matter if you are a girl or not,
[SPEAKER_00]: can contribute a cell phone image with a
specific guidance that would allow us to
[SPEAKER_00]: build AI models to better understand the
regional level thing without much of a
[SPEAKER_00]: pressure lacking of data.
[SPEAKER_00]: So I hope to share more maybe in the
coming years or two, but so far everyone
[SPEAKER_00]: we reach out is very excited about this
approach because they now feel being part
[SPEAKER_00]: of agriculture rather than, oh yeah,
I'm just a buyer of the agricultural
[SPEAKER_00]: produces period, right?
[SPEAKER_00]: So if you are part of something,
then you start to generate a different
[SPEAKER_00]: motivation and passion.
[SPEAKER_00]: So with that, I gonna acknowledge for all
these funding agencies, programs to really
[SPEAKER_00]: support all this wonderful work and also
thank everyone in actually the beginning
[SPEAKER_00]: picture from the four labs who made
significant contributions otherwise I
[SPEAKER_00]: won't give them the talk here today.
[SPEAKER_00]: With that, I hope I still have some time
to take questions.
[SPEAKER_00]: Thanks.
[SPEAKER_02]: Yeah.
[SPEAKER_02]: I was just curious, you said that the
robot was around $35,000 now, I think.
[SPEAKER_02]: Did you compare with how much it costs for
the ground truth by humans?
[SPEAKER_02]: What is the cost of that?
[SPEAKER_00]: So the question is the robot costs $35,000
and what's the cost for human scouting for
[SPEAKER_00]: the equivalent coverage and accuracy?
[SPEAKER_00]: That's a great question and we did not
really do a very precise calculation on
[SPEAKER_00]: that kind of a category because largely we
tested mostly for the research manures and
[SPEAKER_00]: that's being paid by Katie's program.
[SPEAKER_00]: I'm not sure how much dollars you put for
that, but I guess that's a kind of a
[SPEAKER_00]: significant portion of your program,
right?
[SPEAKER_00]: Being higher people and doing that.
[SPEAKER_00]: So I would consider a robot still might be
cheaper, right?
[SPEAKER_00]: Might be cheaper in my opinion,
but again, technology itself, the robot
[SPEAKER_00]: itself is there, right?
[SPEAKER_00]: But so far we need the operator to learn
what is Linux.
[SPEAKER_00]: How many of you use Linux on daily basis?
[SPEAKER_00]: Right, yeah, okay.
[SPEAKER_00]: I saw several people and there are some
kind of a troubleshooting skills we need
[SPEAKER_00]: for the operator.
[SPEAKER_00]: What if something happened and how you can
really quickly put the robot back in
[SPEAKER_00]: function?
[SPEAKER_00]: So those are steep learning, maybe not
steep, but at least some learning curves
[SPEAKER_00]: there to prepare a new workforce and
that's a reason why through my extension
[SPEAKER_00]: actually we put a lot of efforts now try
to distribute needed training materials
[SPEAKER_00]: for the growers, but at the same time
working with different sectors,
[SPEAKER_00]: try to lever or create the synergy to
build a new workforce development
[SPEAKER_00]: programs, especially with Cornell as a so
unique position with all of these aspects.
[SPEAKER_04]: Just a math real quick.
[SPEAKER_04]: So for just the training data for the
whole season, it was approximately $12,000
[SPEAKER_04]: for sub one acre.
[SPEAKER_04]: So as soon as you're using the robot in
its current condition over three acres,
[SPEAKER_04]: you're netting.
[SPEAKER_04]: And that doesn't count like indirect
fault.
[SPEAKER_00]: Oh, yes.
[SPEAKER_05]: I have a follow up question to Bills about
cost to the robot.
[SPEAKER_05]: It also relates to resolution of the
satellite and relates also to like the
[SPEAKER_05]: potential of the imaging technology for
precision out agriculture.
[SPEAKER_05]: So unless I'm mistaken, a lot of growers
are in your field on a regular basis
[SPEAKER_05]: driving your equipment.
[SPEAKER_05]: How much of the cost of the robot is
navigation, right?
[SPEAKER_05]: Relative to image acquisition and data
analysis.
[SPEAKER_05]: Take the guts of the robot and put it on
the track and make something.
[SPEAKER_05]: And is the image analysis fast or is this
a realistic goal?
[SPEAKER_05]: So that you put it on the front of the
tractor and the images are analyzed by the
[SPEAKER_05]: time the back of the tractor passes and
then this nozzle comes on or off.
[SPEAKER_00]: So let me summarize your question.
[SPEAKER_00]: It's like how about to translate some
components from the current robot
[SPEAKER_00]: prototype into the existing agricultural
machinery so that we can lower down the
[SPEAKER_00]: operation cost while still taking
advantage of all this advanced technology,
[SPEAKER_00]: right?
[SPEAKER_00]: Okay, great.
[SPEAKER_00]: And that's a great suggestion.
[SPEAKER_00]: And I agree with you 100% about that.
[SPEAKER_00]: And I'm not sure if you notice when we
distribute the robots to different places,
[SPEAKER_00]: we do not distribute the whole prototype.
[SPEAKER_00]: We actually take out the images kind of
component or module and really distribute
[SPEAKER_00]: that part because when we consider
autonomous scouting, right?
[SPEAKER_00]: Autonomous is one thing that's more like
about the navigation and people may not
[SPEAKER_00]: really enjoy or like need that like a
$15,000 each navigation module.
[SPEAKER_00]: But the much more like the, you know,
$20,000 imaging module with analysis
[SPEAKER_00]: capacity, right?
[SPEAKER_00]: And I agree with you.
[SPEAKER_00]: Then we can attach to ATV, tractor,
any of the like equipment nowadays we can
[SPEAKER_00]: easily find in the vineyards or orchard or
other kind of farms so that people can
[SPEAKER_00]: start to gain the benefits.
[SPEAKER_00]: One thing I want kind of a highlight is
actually the analysis power, right?
[SPEAKER_00]: Or the speed or turnaround time of getting
the raw data to the result the growers can
[SPEAKER_00]: use or researchers can use is getting much
faster.
[SPEAKER_00]: And my personal kind of vision is to this
thing is going to improve infinitely,
[SPEAKER_00]: right?
[SPEAKER_00]: And even for our current robots is running
on a near real-time term, which means as
[SPEAKER_00]: soon as the camera taking one image,
the analysis module finished the inference
[SPEAKER_00]: at the same time.
[SPEAKER_00]: And then you will just keep continue.
[SPEAKER_00]: So at the end of the scanning of the
robot, we also start to have the map of
[SPEAKER_00]: where the diseases are in that kind of a
particular field and it's all geo
[SPEAKER_00]: referenced.
[SPEAKER_00]: So then back to our last point,
that really kind of enable us for many of
[SPEAKER_00]: these preceding and management practices,
right?
[SPEAKER_00]: And then we can use for target spray or
even using the current fan base that still
[SPEAKER_00]: can see a lot of chemicals, which is also
a concern right now.
[SPEAKER_00]: But in the long run, I would see having
this robot might also help us to manage
[SPEAKER_00]: the soil problems in the long run.
[SPEAKER_00]: Because the large ag equipment cause a lot
of soil compaction and there is no kind of
[SPEAKER_00]: a good solution, right?
[SPEAKER_00]: There might be additional motivation and
also because of the cost, if that's kind
[SPEAKER_00]: of a compact system, growers may just
share that unit and just load on their
[SPEAKER_00]: trailer and walk around so several people
can just buy one rather than every of them
[SPEAKER_00]: will buy one.
[SPEAKER_00]: Yeah, I hope I answered your question in a
more comprehensive way.
[SPEAKER_00]: Yeah, yes, yes.
[SPEAKER_02]: So are you able to differentiate different
diseases?
[SPEAKER_00]: Great question.
[SPEAKER_00]: That's a wonderful question.
[SPEAKER_00]: So at the beginning, we solely focus on
either downy or pottery or GRRV.
[SPEAKER_00]: And last year for that kind of a
sensitivity slice, if you still remember,
[SPEAKER_00]: robot is better kind of identified small
spot.
[SPEAKER_00]: That's actually a mixture of downy and
GRRV.
[SPEAKER_00]: So we will be able to differentiate that.
[SPEAKER_00]: But largely, my kind of personal opinion
is I really use human pathology as the
[SPEAKER_00]: standard.
[SPEAKER_00]: What human can do?
[SPEAKER_00]: Let's don't really kind of consider more
advanced sensing technologies because that
[SPEAKER_00]: always give us advantages, right?
[SPEAKER_00]: But let's step back.
[SPEAKER_00]: You and I just have this naked eye just
for the visible range to see all these
[SPEAKER_00]: things in our world.
[SPEAKER_00]: But with your knowledge, you still be able
to differentiate a lot of these things.
[SPEAKER_00]: And my current goal is how we can push
robots and AI to achieve that first.
[SPEAKER_00]: And of course, if we can achieve that with
advanced sensing modules with additional
[SPEAKER_00]: information chain, everything going to be
just boosted naturally.
[SPEAKER_04]: I can add as well.
[SPEAKER_04]: So a lot of people from my group have
spent a lot of time training a robot.
[SPEAKER_04]: In particular, my field research manager,
Dave Holmes, who's forgotten more about
[SPEAKER_04]: disease management than I think any of us
really hope to know.
[SPEAKER_04]: The robot, the new model, is just as good
at looking at its imagery as he is.
[SPEAKER_04]: So there's still room for him in the
field, sticking his head in the vine,
[SPEAKER_04]: being there.
[SPEAKER_04]: But looking at the imagery, they're
equivalent.
[SPEAKER_04]: And I think that gets to use point where
it's the human pathology.
[SPEAKER_04]: You want to repeat that to the Zoom
audience?
[SPEAKER_00]: Oh, yeah.
[SPEAKER_00]: On Zoom, what Katie can add in is now the
robot is as good as her very long-term and
[SPEAKER_00]: wonderful technician, Dave Cobb.
[SPEAKER_00]: With 30 years of experience about the
disease in group wise.
[SPEAKER_00]: Thanks so much for the comment.
[SPEAKER_00]: Oh, yes.
[SPEAKER_00]: Sorry.
[SPEAKER_00]: I got that.
[SPEAKER_00]: Yes and yes.
[SPEAKER_03]: Is the ground imaging dependent on fair
weather days?
[SPEAKER_03]: Or is it restricted by very bright days,
like post rain, or is it overhead
[SPEAKER_03]: irrigation due to face limits like that?
[SPEAKER_00]: Yeah.
[SPEAKER_00]: The question is whether or not the robot
is being limited by environment conditions
[SPEAKER_00]: like the sunlight or varying conditions.
[SPEAKER_00]: So great question.
[SPEAKER_00]: So far, the only kind of enemy for the
robot is the severe weather events,
[SPEAKER_00]: like very windy, very snowy, very rainy.
[SPEAKER_00]: I cannot run that because nothing is
weather proof.
[SPEAKER_00]: But other than that, I did not introduce
more on the sensing side, but we use
[SPEAKER_00]: active illumination to combat the
sunlight.
[SPEAKER_00]: So the camera does not rely on the natural
light to provide the information.
[SPEAKER_00]: It actually relies on artificial light.
[SPEAKER_00]: So that only that light provides the
illumination.
[SPEAKER_00]: And that's how we really try to control
the data quality consistency and actually
[SPEAKER_00]: also boost the AI model for the
generalized capability.
[SPEAKER_00]: So it serves both purposes.
[SPEAKER_00]: And it can run day and night.
[SPEAKER_00]: Yes.
[SPEAKER_01]: My question was about what powers the
robot.
[SPEAKER_01]: And it's a charge.
[SPEAKER_01]: How long can it go without feeding a
charge?
[SPEAKER_01]: I'm thinking it's in the middle of a
vineyard.
[SPEAKER_01]: It might be hard to go back and charge it.
[SPEAKER_00]: Yeah.
[SPEAKER_00]: So the question is about the battery
consumption or the power consumption and
[SPEAKER_00]: the charging time for the robot.
[SPEAKER_00]: So far, for the robot, it can constantly
run about three hours for one charge.
[SPEAKER_00]: And usually, we feel that's kind of
sufficient to cover it is two acres filled
[SPEAKER_00]: per charge.
[SPEAKER_00]: But of course, we are upgrading all the
platform and improve the battery
[SPEAKER_00]: technology.
[SPEAKER_00]: And I know at Cornell, we have a lot of
researchers on the energy side.
[SPEAKER_00]: We hope actually to further improve that
down the road.
[SPEAKER_00]: Yes.
[SPEAKER_02]: Did you have a question?
[SPEAKER_02]: Is that missing?
[SPEAKER_00]: Oh, I missed that.
[SPEAKER_00]: Let me open that.
[SPEAKER_00]: Oh, there are several messages on the
Zoom.
[SPEAKER_00]: Sorry, folks.
[SPEAKER_00]: Just open that.
[SPEAKER_00]: First comment from Gary.
[SPEAKER_00]: Very exciting.
[SPEAKER_00]: Permission research.
[SPEAKER_00]: Well communicated.
[SPEAKER_00]: Thanks.
[SPEAKER_00]: Thanks, Gary, for you.
[SPEAKER_00]: Second from Edwin.
[SPEAKER_00]: I'm wondering what is the maximum height
that the robot can extend?
[SPEAKER_00]: Can it work on hops up to six meters tall?
[SPEAKER_00]: So, Ed, that's a great question.
[SPEAKER_00]: So so far, it's not on this kind of a
robot-based platform.
[SPEAKER_00]: We have a new one.
[SPEAKER_00]: And that one can be used for apples,
although for a different purpose,
[SPEAKER_00]: not for disease at the moment.
[SPEAKER_00]: It's more like for its 3D structure for
the digital twin purpose.
[SPEAKER_00]: So I would say six meters might be OK.
[SPEAKER_00]: It depends on the base, right?
[SPEAKER_00]: Everything is down to the mechanical
designing, whether or not it's stable.
[SPEAKER_00]: And also, a challenge for New York forever
is our candidate.
[SPEAKER_00]: It's kind of a terrain situation.
[SPEAKER_00]: If that's in Oregon, Washington,
California, with a super flat field,
[SPEAKER_00]: I would say, oh, yeah, let's just do that.
[SPEAKER_00]: But now with New York, we have all these
slopes, all these things.
[SPEAKER_00]: I think it's kind of a very case-by-case
at this moment.
[SPEAKER_00]: But I think in January, six meters would
be OK.
[SPEAKER_00]: Yeah.
[SPEAKER_00]: Hope to answer your question.
[SPEAKER_00]: Next from Luke.
[SPEAKER_00]: Do you think it will be on the individual
farmer to buy into this remote sensing
[SPEAKER_00]: technology to use it?
[SPEAKER_00]: Or do you foresee that there will be a
subscription service, perhaps where a
[SPEAKER_00]: company will have a fleet of drones,
rovers, sensors, extra, and be able to
[SPEAKER_00]: work with farmers to do the data capture
to data processing and make
[SPEAKER_00]: recommendations on decision making?
[SPEAKER_00]: Wonderful question, Luke.
[SPEAKER_00]: So that's also my feeling.
[SPEAKER_00]: I don't really have a very strong opinion
either way.
[SPEAKER_00]: But I think both can work.
[SPEAKER_00]: And I see definitely both work.
[SPEAKER_00]: Because for many of the pilot growers who
kind of run very large vineyard,
[SPEAKER_00]: generate tons of their revenues,
their only question is how I can maintain
[SPEAKER_00]: that by saving money.
[SPEAKER_00]: That's their kind of a first priority.
[SPEAKER_00]: And I wouldn't see they would be hesitant
if this is to prove the technology,
[SPEAKER_00]: they're going to invest on that,
and then really use that on a daily basis.
[SPEAKER_00]: But of course, then down to small to
medium size, the farmers, that's a totally
[SPEAKER_00]: different world.
[SPEAKER_00]: And sometimes even asking for $500
subscription fee can be a glitchy question
[SPEAKER_00]: there.
[SPEAKER_00]: So I definitely foresee both business
model could wrong.
[SPEAKER_00]: And I think the best way is let's just
explore and see how these things go
[SPEAKER_00]: naturally.
[SPEAKER_00]: Because in my opinion, what we bring in
for this closed loop framework is to let
[SPEAKER_00]: the pilot and the large growers to
actually help the small to medium size
[SPEAKER_00]: farmers.
[SPEAKER_00]: Because largely, the training data set is
going to be contributed by them.
[SPEAKER_00]: And then the knowledge can be translated
to the other group who might not be able
[SPEAKER_00]: to afford this advanced technology at the
moment.
[SPEAKER_00]: Last from Donna, hi you for virus
detection.
[SPEAKER_00]: The best use for remote sensing might be
in the nursery.
[SPEAKER_00]: Do you have any plans to use the robot or
imaging models in a commercial grip on
[SPEAKER_00]: nursery?
[SPEAKER_00]: Wonderful question.
[SPEAKER_00]: Donna, maybe you can just shoot me an
email.
[SPEAKER_00]: Let's just chat after this seminar.
[SPEAKER_00]: It sounds like a very interesting use case
scenario.
[SPEAKER_00]: That's what I got from Zoom.
[SPEAKER_03]: All right, thanks everyone.
[SPEAKER_03]: This has been a production of Cornell
University on the web at cornell.edu.
[SPEAKER_03]: Thank you.
[SPEAKER_03]: Thank you.
[SPEAKER_03]: Thank you.
Thank you.
